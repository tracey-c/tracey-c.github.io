<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>snn流程</title>
      <link href="/article/hadoop03%20snn%E6%B5%81%E7%A8%8B/"/>
      <url>/article/hadoop03%20snn%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h4 id="1-snn流程">1 snn流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 current]$ pwd</span><br><span class="line">/home/hadoop/tmp/dfs/name/current</span><br><span class="line">[hadoop@hadoop01 current]$ ll</span><br><span class="line"></span><br><span class="line">May 14 18:38 edits_0000000000000021201-0000000000000021202</span><br><span class="line">May 14 19:38 edits_0000000000000021203-0000000000000021204</span><br><span class="line">May 14 20:38 edits_0000000000000021205-0000000000000021206</span><br><span class="line">May 14 21:38 edits_0000000000000021207-0000000000000021208</span><br><span class="line">May 14 22:39 edits_0000000000000021209-0000000000000021210</span><br><span class="line">May 14 23:39 edits_0000000000000021211-0000000000000021212</span><br><span class="line">May 15 00:39 edits_0000000000000021213-0000000000000021217</span><br><span class="line">May 15 01:39 edits_0000000000000021218-0000000000000021219</span><br><span class="line">May 15 02:39 edits_0000000000000021220-0000000000000021221</span><br><span class="line">May 15 03:39 edits_0000000000000021222-0000000000000021223</span><br><span class="line">May 15 04:39 edits_0000000000000021224-0000000000000021225</span><br><span class="line">May 15 05:40 edits_0000000000000021226-0000000000000021227</span><br><span class="line">May 15 06:40 edits_0000000000000021228-0000000000000021229</span><br><span class="line">May 15 07:40 edits_0000000000000021230-0000000000000021231</span><br><span class="line">May 15 08:40 edits_0000000000000021232-0000000000000021233</span><br><span class="line">May 15 08:40 edits_inprogress_0000000000000021234</span><br><span class="line">May 15 07:40 fsimage_0000000000000021231</span><br><span class="line">May 15 07:40 fsimage_0000000000000021231.md5</span><br><span class="line">May 15 08:41 fsimage_0000000000000021233</span><br><span class="line">May 15 08:41 fsimage_0000000000000021233.md5</span><br><span class="line">May 15 08:40 seen_txid</span><br><span class="line">May 14 15:36 VERSION</span><br></pre></td></tr></table></figure><p>合并流程如图<br><img src="/img/hadoop_img/snn%E5%90%88%E5%B9%B6.png" alt="avatar"></p><p>说明：<br>  1）SNN拷贝NN的日志文件edits21232-21233和镜像文件fsimage21231自己服务器中合并生成合并为镜像文件fsimage21233<br>  2）SNN将合并后镜像文件fsimage21233传送给NN，NN的中edits21232-21233和镜像文件fsimage21231被拷贝走生成edit操作，写入edit21234 inprogress接收新来的数据</p><p>snn流程概述<br>NameNode 的元数据信息先往 edits 文件中写，当 edits 文件达到一定的阈值(3600 秒) 的时候，会开启合并的流程。<br>**<strong><em>合并流程:</em></strong><br>  1）当开始合并的时候，SecondaryNameNode 会把 edits 和 fsimage 拷贝到自己服务器所在内存中，开始合并，合并生成一个名为 fsimage.ckpt 的文件。<br>  2）将 fsimage.ckpt 文件拷贝到 NameNode 上，成功后，再删除原有的 fsimage，并 将 fsimage.ckpt 文件重命名为 fsimage。<br>  3）当 SecondaryNameNode 将 edits 和 fsimage 拷贝走之后，NameNode 会立刻生成 一个 edits.new 文件，用于记录新来的元数据，当合并完成之后，原有的 edits 文件才会被 删除，并将 edits.new 文件重命名为 edits 文件，开启下一轮流程。</p>]]></content>
      
      
      
        <tags>
            
            <tag> snn流程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS架构</title>
      <link href="/article/HDFS%E6%9E%B6%E6%9E%84/"/>
      <url>/article/HDFS%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h4 id="1-hdfs主从架构">1 HDFS主从架构</h4><p><img src="/img/hadoop_img/hdfs%E6%9E%B6%E6%9E%84.png" alt="avatar"></p><h4 id="11-namenode-名称节点-nn">1.1 namenode  名称节点 nn</h4><ul><li>a.文件名称</li><li>b.文件的目录结构</li><li>c.文件的属性 权限 创建时间 副本数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 ~]$ hdfs dfs -ls  /wordcount2/input/</span><br><span class="line">19/10/18 20:12:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         35 2019-10-18 21:34 /wordcount2/input/1.log</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         29 2019-10-18 21:34 /wordcount2/input/2.log</span><br><span class="line">[hadoop@hadoop01 ~]$</span><br></pre></td></tr></table></figure><ul><li>d.一个文件被对应切割哪些数据块 副本数的块  9块===》数据块对应分布在哪些节点上<br>blockmap 块映射  当然nn节点是不会持久化存储这种映射关系<br>是通过集群的启动和运行时，dn定期发送blockreport给nn，然后nn就在内存中动态维护这种映射关系</li></ul><p>namenode作用:<br>管理文件系统的命名空间  其实就是维护文件系统树的文件和文件夹这些形式是以两种文件来永久的保存在本地磁盘的<br>镜像文件 fsimage<br>编辑日志文件 editlogs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 current]$ pwd</span><br><span class="line">/home/hadoop/tmp/dfs/name/current</span><br><span class="line">[hadoop@hadoop01 current]$ ll</span><br><span class="line">edits_0000000000000000375-0000000000000000376</span><br><span class="line">edits_0000000000000000377-0000000000000000378</span><br><span class="line">edits_0000000000000000379-0000000000000000380</span><br><span class="line">edits_0000000000000000381-0000000000000000382</span><br><span class="line">edits_0000000000000000383-0000000000000000384</span><br><span class="line">edits_0000000000000000385-0000000000000000386</span><br><span class="line">edits_inprogress_0000000000000000387</span><br><span class="line">fsimage_0000000000000000384</span><br><span class="line">fsimage_0000000000000000384.md5</span><br><span class="line">fsimage_0000000000000000386</span><br><span class="line">fsimage_0000000000000000386.md5</span><br></pre></td></tr></table></figure><h6 id="12-secondary-namenode-第二名称节点-snn">1.2 secondary namenode  第二名称节点  snn</h6><ul><li>a.fsimage editlog定期拿过来合并 备份 推送</li></ul> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 current]$ pwd</span><br><span class="line">/home/hadoop/tmp/dfs/namesecondary/current</span><br><span class="line">[hadoop@hadoop01 current]$ </span><br><span class="line">edits_0000000000000000377-0000000000000000378</span><br><span class="line">edits_0000000000000000379-0000000000000000380</span><br><span class="line">edits_0000000000000000381-0000000000000000382</span><br><span class="line">edits_0000000000000000383-0000000000000000384</span><br><span class="line">edits_0000000000000000385-0000000000000000386</span><br><span class="line">fsimage_0000000000000000384</span><br><span class="line">fsimage_0000000000000000384.md5</span><br><span class="line">fsimage_0000000000000000386</span><br><span class="line">fsimage_0000000000000000386.md5</span><br><span class="line">             </span><br><span class="line">fsimage_0000000000000000384</span><br><span class="line">snn将老大的  edits_0000000000000000385-0000000000000000386   ==》检查点动作 checkpoint  合并为</span><br><span class="line">             </span><br><span class="line">fsimage_0000000000000000386 将这个386文件推送给老大，那么新的数据写读的记录就存放在edits_inprogress_0000000000000000387 日志文件  是变化的 追加</span><br></pre></td></tr></table></figure><pre><code>    dfs.namenode.checkpoint.period  3600s     dfs.namenode.checkpoint.txns    1000000  为了解决单点故障，nn只有一个对外的，后来新增一个snn，1小时的备份 虽然能够减轻单点故障带来的丢失风险，但是在生产上还是不允许使用snn 11:00  snn 备份 11:30  数据一直写到这  突然nn节点 磁盘故障 无法恢复 拿snn的最新的一个fsimage文件恢复，那么只能恢复 11点的数据 在生产上是 不用snn，是启动另外一个NN进程(实时备份，实时准备替换nn，变为活动的nn)    叫做HDFS HA</code></pre><h6 id="13-datanode-数据节点-dn">1.3 datanode  数据节点  dn</h6><ul><li>a.存储数据块和数据块的校验和</li></ul><p>datanode作用:<br>1）每隔3s发送心跳给nn,告诉 我还活着<br>dfs.heartbeat.interval  3s</p><p>2）每隔一定的时间发生一次blockreport<br>dfs.blockreport.intervalMsec   21600000ms=6小时<br>dfs.datanode.directoryscan.interval  21600s=6小时</p>]]></content>
      
      
      
        <tags>
            
            <tag> HDFS架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS副本放置策略</title>
      <link href="/article/HDFS%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/"/>
      <url>/article/HDFS%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<p>1.HDFS副本放置策略<br><img src="/img/hadoop_img/hdfs%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5.png" alt="avatar"><br>机架 rack1 5   rack2 5</p><p>生产上读写操作 尽量选择 某个DN节点<br>第一个副本：<br>放置在上传的DN节点；<br>假如是非DN节点，就随机挑选一个磁盘不太慢，cpu不太忙的节点；</p><p>第二个副本：<br>放置在第一个副本不同的机架上的某个DN节点。</p><p>第三个副本:<br>与第二个副本相同机架的不同节点上。</p><p>如果副本数设置更多，就随机放。</p>]]></content>
      
      
      
        <tags>
            
            <tag> HDFS副本放置策略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop02词频统计、pid、副本</title>
      <link href="/article/hadoop02%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1%E3%80%81pid%E3%80%81%E5%89%AF%E6%9C%AC%E6%95%B0/"/>
      <url>/article/hadoop02%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1%E3%80%81pid%E3%80%81%E5%89%AF%E6%9C%AC%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h4 id="1词频统计">1.词频统计</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 data]$ hdfs dfs -mkdir  /wordcount2</span><br><span class="line">19/10/09 21:33:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop01 data]$ </span><br><span class="line">[hadoop@hadoop01 data]$ hdfs dfs -mkdir  /wordcount2/input/</span><br><span class="line">19/10/09 21:33:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop01 data]$ hdfs dfs -put * /wordcount2/input/</span><br><span class="line">19/10/09 21:34:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop01 data]$ hdfs dfs -ls  /wordcount2/input/</span><br><span class="line">19/10/09 21:34:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         35 2019-10-09 21:34 /wordcount2/input/1.log</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         29 2019-10-09 21:34 /wordcount2/input/2.log</span><br><span class="line">[hadoop@hadoop01 data]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop01 hadoop]$ hadoop jar \</span><br><span class="line">./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar  \</span><br><span class="line">wordcount /wordcount2/input/ /wordcount2/output1/</span><br><span class="line"></span><br><span class="line">19/10/09 21:38:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/09 21:38:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">19/10/09 21:38:09 INFO input.FileInputFormat: Total input paths to process : 2</span><br><span class="line"></span><br><span class="line">19/10/09 21:38:10 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">#思考: mapreduce的split=map的个数 是什么影响的</span><br><span class="line">MapReduce作业map任务的个数是根据输入文件的分片（split）个数决定的，一个split对应一个map任务。</span><br><span class="line">控制 map 数量：  </span><br><span class="line">新版本 mapreduce 的 textInputFormat 使用参数： </span><br><span class="line">mapreduce.input.fileinputformat.split.maxsize和mapreduce.input.fileinputformat.split.minsize </span><br><span class="line">来控制 split 也就是 map 的数量  </span><br><span class="line">公式：split_size = max(minsize,min(maxsize,blocksize)) 默认 spilt_size=blocksize   </span><br><span class="line">默认 maxsize 为 Long.MaxValue,minsize 为 0，所以默认 map 大小等于 blocksize， 也就是 128M  </span><br><span class="line">如果要增多 map 数量，就将 maxsize 的值设置比 blocksize 小  </span><br><span class="line">如果要减少 map 数量，就将 minsize 的值设置比 blocksize 大 </span><br><span class="line"></span><br><span class="line">19/10/09 21:38:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1589030910472_0001</span><br><span class="line">19/10/09 21:38:10 INFO impl.YarnClientImpl: Submitted application application_1589030910472_0001</span><br><span class="line">19/10/09 21:38:10 INFO mapreduce.Job: The url to track the job: http://hadoop01:18088/proxy/application_1589030910472_0001/</span><br><span class="line">19/10/09 21:38:10 INFO mapreduce.Job: Running job: job_1589030910572_0001</span><br><span class="line">19/10/09 21:38:17 INFO mapreduce.Job: Job job_1589030910472_0001 running in uber mode : false</span><br><span class="line">19/10/09 21:38:17 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/10/09 21:38:22 INFO mapreduce.Job:  map 50% reduce 0%</span><br><span class="line">19/10/09 21:38:23 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/10/09 21:38:28 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/10/09 21:38:28 INFO mapreduce.Job: Job job_1589030910472_0001 completed successfully</span><br><span class="line">19/10/09 21:38:28 INFO mapreduce.Job: Counters: 49</span><br><span class="line">        File System Counters</span><br><span class="line">                FILE: Number of bytes read=116</span><br><span class="line">                FILE: Number of bytes written=429291</span><br><span class="line">                FILE: Number of read operations=0</span><br><span class="line">                FILE: Number of large read operations=0</span><br><span class="line">                FILE: Number of write operations=0</span><br><span class="line">                HDFS: Number of bytes read=288</span><br><span class="line">                HDFS: Number of bytes written=41</span><br><span class="line">                HDFS: Number of read operations=9</span><br><span class="line">                HDFS: Number of large read operations=0</span><br><span class="line">                HDFS: Number of write operations=2</span><br><span class="line">        Job Counters </span><br><span class="line">                Launched map tasks=2</span><br><span class="line">                Launched reduce tasks=1</span><br><span class="line">                Data-local map tasks=2</span><br><span class="line">                Total time spent by all maps in occupied slots (ms)=6455</span><br><span class="line">                Total time spent by all reduces in occupied slots (ms)=2812</span><br><span class="line">                Total time spent by all map tasks (ms)=6455</span><br><span class="line">                Total time spent by all reduce tasks (ms)=2812</span><br><span class="line">                Total vcore-milliseconds taken by all map tasks=6455</span><br><span class="line">                Total vcore-milliseconds taken by all reduce tasks=2812</span><br><span class="line">                Total megabyte-milliseconds taken by all map tasks=6609919</span><br><span class="line">                Total megabyte-milliseconds taken by all reduce tasks=2879488</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">                Map input records=7</span><br><span class="line">                Map output records=16</span><br><span class="line">                Map output bytes=127</span><br><span class="line">                Map output materialized bytes=122</span><br><span class="line">                Input split bytes=224</span><br><span class="line">                Combine input records=16</span><br><span class="line">                Combine output records=11</span><br><span class="line">                Reduce input groups=7</span><br><span class="line">                Reduce shuffle bytes=122</span><br><span class="line">                Reduce input records=11</span><br><span class="line">                Reduce output records=7</span><br><span class="line">                Spilled Records=22</span><br><span class="line">                Shuffled Maps =2</span><br><span class="line">                Failed Shuffles=0</span><br><span class="line">                Merged Map outputs=2</span><br><span class="line">                GC time elapsed (ms)=170</span><br><span class="line">                CPU time spent (ms)=1350</span><br><span class="line">                Physical memory (bytes) snapshot=781856768</span><br><span class="line">                Virtual memory (bytes) snapshot=8315179008</span><br><span class="line">                Total committed heap usage (bytes)=770179072</span><br><span class="line">        Shuffle Errors</span><br><span class="line">                BAD_ID=0</span><br><span class="line">                CONNECTION=0</span><br><span class="line">                IO_ERROR=0</span><br><span class="line">                WRONG_LENGTH=0</span><br><span class="line">                WRONG_MAP=0</span><br><span class="line">                WRONG_REDUCE=0</span><br><span class="line">        File Input Format Counters </span><br><span class="line">                Bytes Read=64</span><br><span class="line">        File Output Format Counters </span><br><span class="line">                Bytes Written=41</span><br><span class="line">[hadoop@hadoop01 hadoop]$ </span><br><span class="line"></span><br><span class="line">[hadoop@hadoop01 hadoop]$ hdfs dfs -ls /wordcount2/output1/</span><br><span class="line">19/10/09 21:41:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2019-10-09 21:38 /wordcount2/output1/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         41 2019-10-09 21:38 /wordcount2/output1/part-r-00000</span><br><span class="line">[hadoop@hadoop01 hadoop]$ </span><br><span class="line"></span><br><span class="line">#思考:mapreduce跑完结束，文件的个数=reduce由什么决定，当前是1</span><br><span class="line">part-r-00000</span><br><span class="line">reduce个数受partition影响，一个partition对应一个reduce输入</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop01 hadoop]$ cat part-r-00000</span><br><span class="line">a       4</span><br><span class="line">ab      1</span><br><span class="line">b       1</span><br><span class="line">hive    3</span><br><span class="line">data    1</span><br><span class="line">jacket  2</span><br><span class="line">hadoop  3</span><br></pre></td></tr></table></figure><h4 id="2jps命令">2.jps命令</h4><h6 id="21-位置">2.1 位置</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 hadoop]$ which jps</span><br><span class="line">/usr/java/jdk1.8.0_181/bin/jps</span><br></pre></td></tr></table></figure><h6 id="22-使用">2.2 使用</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 hadoop]$ jps</span><br><span class="line">21712 DataNode</span><br><span class="line">21585 NameNode</span><br><span class="line">23989 ResourceManager</span><br><span class="line">29877 Jps</span><br><span class="line">24094 NodeManager</span><br><span class="line">21871 SecondaryNameNode</span><br></pre></td></tr></table></figure><h6 id="23-对应的进程标识文件存储在哪">2.3 对应的进程标识文件存储在哪</h6><p>/tmp/hsperfdata_hadoop</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 tmp]$ cd hsperfdata_hadoop</span><br><span class="line">[hadoop@hadoop01 hsperfdata_hadoop]$ ll</span><br><span class="line">total 160</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 May  9 22:01 21585</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 May  9 22:01 21712</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 May  9 22:01 21871</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 May  9 22:01 23989</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 May  9 22:01 24094</span><br><span class="line">[hadoop@hadoop01 hsperfdata_hadoop]$ mv 21712 21712.bak  #mv同个目录备份再移回会丢失 移动其他目录没问题如：mv 21712 /tmp/</span><br><span class="line">[hadoop@hadoop01 hsperfdata_hadoop]$ </span><br><span class="line">[hadoop@hadoop01 hsperfdata_hadoop]$ ps -ef|grep DataNode</span><br><span class="line">hadoop   21712     1  0 May06 ?        00:03:54 /usr/java/jdk1.8.0_181/bin/java \</span><br><span class="line">-Dproc_datanode -Xmx1000m \</span><br><span class="line">-Djava.net.preferIPv4Stack=true \</span><br><span class="line">-Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs </span><br><span class="line">-Dhadoop.log.file=hadoop.log \</span><br><span class="line">-Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2 \</span><br><span class="line">-Dhadoop.id.str=hadoop \</span><br><span class="line">-Dhadoop.root.logger=INFO,console \</span><br><span class="line">-Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/lib/native \</span><br><span class="line">-Dhadoop.policy.file=hadoop-policy.xml \</span><br><span class="line">-Djava.net.preferIPv4Stack=true \</span><br><span class="line">-Djava.net.preferIPv4Stack=true \</span><br><span class="line">-Djava.net.preferIPv4Stack=true \</span><br><span class="line">-Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs </span><br><span class="line">-Dhadoop.log.file=hadoop-hadoop-datanode-hadoop01.log </span><br><span class="line">-Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2 \</span><br><span class="line">-Dhadoop.id.str=hadoop \</span><br><span class="line">-Dhadoop.root.logger=INFO,RFA \</span><br><span class="line">-Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/lib/native \</span><br><span class="line">-Dhadoop.policy.file=hadoop-policy.xml \</span><br><span class="line">-Djava.net.preferIPv4Stack=true -server \</span><br><span class="line">-Dhadoop.security.logger=ERROR,RFAS \</span><br><span class="line">-Dhadoop.security.logger=ERROR,RFAS \</span><br><span class="line">-Dhadoop.security.logger=ERROR,RFAS \</span><br><span class="line">-Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">hadoop   30469 30251  0 22:03 pts/1    00:00:00 grep --color=auto DataNode</span><br><span class="line">[hadoop@hadoop01 hsperfdata_hadoop]$ </span><br><span class="line"></span><br><span class="line">[hadoop@hadoop01 hadoop]$ jps</span><br><span class="line">21585 NameNode</span><br><span class="line">23989 ResourceManager</span><br><span class="line">30358 Jps</span><br><span class="line">24094 NodeManager</span><br><span class="line">21871 SecondaryNameNode</span><br><span class="line">[hadoop@hadoop01 hadoop]$</span><br></pre></td></tr></table></figure><h6 id="24-当出现-process-information-unavailable">2.4 当出现 – process information unavailable</h6><p>进程不能代表是存在 或者 不存在，要当心，尤其是使用jps检测状态的<br>ps -ef | grep xxx 命令是去查看进程的真正手段！！！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 ~]# jps</span><br><span class="line">21712 -- process information unavailable</span><br><span class="line">31713 Jps</span><br><span class="line">23989 -- process information unavailable</span><br><span class="line">24094 NodeManager</span><br><span class="line">[root@hadoop01 ~]# ps -ef |grep 23989</span><br><span class="line">root     31791 31644  0 22:11 pts/2    00:00:00 grep --color=auto 23989 #进程不存在</span><br><span class="line">[root@hadoop01 ~]# ps -ef |grep 21712</span><br><span class="line">hadoop   21712     1  0 May06 ?        00:03:54 /usr/java/jdk1.8.0_181/bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs -Dhadoop.log.file=hadoop-hadoop-datanode-hadoop01.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">root     31812 31644  0 22:11 pts/2    00:00:00 grep --color=auto 21712 #进程存在</span><br><span class="line">[root@hadoop01 ~]#</span><br></pre></td></tr></table></figure><h6 id="25-文件被删除不影响进程的重启">2.5 文件被删除，不影响进程的重启！！</h6><p>/tmp/hsperfdata_hadoop 目录下去清理<br>21712 – process information unavailable<br>重新启动 sbin/start-all.sh 不影响进程的重启</p><h4 id="3pid文件">3.pid文件</h4><h6 id="31-pid存储位置">3.1 pid存储位置</h6><p>默认存储在/tmp</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 tmp]# ll</span><br><span class="line">total 19</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop   4 Oct  9 22:16 hadoop-hadoop-datanode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop   4 Oct  9 22:16 hadoop-hadoop-namenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop   5 Oct  9 22:16 hadoop-hadoop-secondarynamenode.pid</span><br></pre></td></tr></table></figure><h6 id="32-维护进程的pid-写死的">3.2 维护进程的pid 写死的</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 tmp]# cat hadoop-hadoop-namenode.pid</span><br><span class="line">792</span><br><span class="line">[root@hadoop01 tmp]#</span><br></pre></td></tr></table></figure><h6 id="33-文件被删除影响进程的重启">3.3 文件被删除，影响进程的重启！！</h6><p>进程启动，pid文件写入进程的pid数字<br>进程关闭时，从pid文件读出pid数字，然后kill -9 pid</p><h6 id="34-生产上pid存放">3.4 生产上pid存放</h6><p>生产场景 pid文件真的可以放心的丢在/tmp维护吗？<br>Linux的/tmp 会有30天的默认删除机制</p><h6 id="35-如何修改">3.5 如何修改</h6><p>修改hadoop-env.sh脚本<br> export HAOOOP_PID_DIR=/home/hadoop/tmp/  #路径自定义写死防止丢失<br>修改后<br>[hadoop@hadoop01 hadoop]$ kill -9 $(pgrep -f hadoop)   #所有进程杀死<br>[hadoop@hadoop01 hadoop]$ ps -ef | grep hadoop #check一下进程<br>[hadoop@hadoop01 hadoop]$ pwd<br>/home/hadoop/app/hadoop/etc/hadoop</p><p>重启进程sbin/start-all.sh查看是在/home/hadoop/tmp/ 下<br>总结：<br>  pid文件生产不要丢在/tmp目录<br>  要知道是影响进程的启动停止</p><h4 id="4块-block">4.块 block</h4><h6 id="41块的概念">4.1块的概念</h6><p>eg:一缸水 260ml<br>瓶子 规格 128ml</p><p>260/128=2…4ml<br>128ml<br>128ml<br>4ml<br>三个瓶子</p><p>HDFS 存储大文件是利好，存储小文件是损害自己的<br>适合存储大文件 不适合小文件，不代表不能存储小文件</p><p>mv  260m文件<br>上传到hdfs,会把文件切割成块 dfs.blocksize 134217728 =128M<br>128m<br>128m<br>4m</p><p>3个块</p><h6 id="42设置-副本数">4.2设置 副本数</h6><p>伪分布式节点 1节点 副本数 dfs.replication 1<br>生产上HDFS集群的 DN肯定大于&gt;=3台   dfs.replication=3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DN1          DN2   DN5  DN10</span><br><span class="line">128m b1            b1   b1</span><br><span class="line">128m b2      b2         b2</span><br><span class="line">4m           b3    b3   b3</span><br></pre></td></tr></table></figure><p>通过设置副本数 来让文件存储在大数据HDFS平台上有容错保障</p><h6 id="43-block大小为什么由64m128m">4.3 block大小为什么由64M–》128M</h6><p>mv  260m文件<br>260/64=4…4M<br>b1 64m<br>b2 64m<br>b3 64m<br>b4 64m<br>b5 4m<br>5个块</p><p>260/128=2…4M<br>128m<br>128m<br>4m<br>3个块</p><p>副本数3：存储实际大小=文件大小<em>3=260</em>3=780m 存储空间</p><p>5个块的元数据信息维护 是不是 比3个块的元数据的信息维护要多，<br>维护重  累–》namenode<br>所以nn是不喜欢 小文件的</p><p>1亿个10kb文件  3亿个block<br>1亿个10kb文件压缩成1KW个文件100m   3KW的block</p><h6 id="44-如何规避-小文件">4.4 如何规避 小文件</h6><p>1）数据传输到hdfs之前就合并<br>2）数据已经在hdfs上，就是定时  业务低谷的 去合并冷文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">      /2019/10/10</span><br><span class="line">/2019/10/11</span><br><span class="line">/2019/10/12  当前时间</span><br></pre></td></tr></table></figure><p>生产上需要定时合并七天前数据</p><p>19号 14号的文件<br>21号 15号的文件<br>一天卡一天</p><p>经验值<br>10m以下的文件 小文件<br>合并到120m文件接近blocksize大小128m（有些数据文件不可切割的）<br>128m  真正合并就超了 变为129m–》2个块  128m  1m<br>会对namenode较大压力<br>10m 10m 10m …10m 12个=119m<br>10m 10m 10m …10m 13个=130m<br>合并 119m文件</p>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop02词频统计、pid、副本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS写流程</title>
      <link href="/article/HDFS%E5%86%99%E6%B5%81%E7%A8%8B/"/>
      <url>/article/HDFS%E5%86%99%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>HDFS读流程如下图<br><img src="hdfs_write.jpg" alt="avatar"></p><p>1.HDFS Client调用FileSystem.create(filePath)方法，去和NN进行【RPC】通信！<br>NN 会去check这个路径的文件是否已经存在，是否有权限能够创建这个文件！<br>假如都ok，就去创建一个新的文件，但是这时还没写数据，是不关联任何的block。<br>nn根据上传的文件大小，根据块大小+副本数参数，计算要上传多少块和块存储在DN的位置。最终将这些信息返回给客户端，为【FSDataOutputStream】对象.</p><p>2.Client调用【FSDataOutputStream】对象的write方法，将第一个块的第一个副本数写第一个DN节点，写完去第二个副本写DN2；写完去第三个副本写DN3。<br>当第三个副本写完，就返回一个ack packet确定包给DN2节点，<br>当DN2节点收到这个ack packet确定包加上自己也是写完了，就返回一个ack packet确定包给第一个DN1节点，当DN1节点收到这个ack packet确定包加上自己也是写完了，<br>将ack packet确定包返回给【FSDataOutputStream】对象，就标识第一个块的三个副本写完。其他块以此类推！</p><p>3.当所有的块全部写完， client调用 【FSDataOutputStream】对象的close方法，<br>关闭输出流。再次调用FileSystem.complete方法，告诉nn文件写成功！</p>]]></content>
      
      
      
        <tags>
            
            <tag> HDFS写流程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS读流程</title>
      <link href="/article/HDFS%E8%AF%BB%E6%B5%81%E7%A8%8B/"/>
      <url>/article/HDFS%E8%AF%BB%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>HDFS读流程如下图<br><img src="hdfs_read.jpg" alt="avatar"></p><p>1.Client调用FileSystem的open(filePath),与NN进行RPC通信，返回该文件的部分或者全部的block列表也就是返回【FSDataInputStream】对象。</p><p>2.Client调用【FSDataInputStream】对象的read方法,去与第一个块的最近的DN进行读取，读取完成后，会check，假如ok，就关闭与DN通信。<br>假如读取失败，会记录DN+block信息，下次就不会从这个节点读取。那么就从第二个节点读取。</p><p>然后去与第二个块的最近的DN进行读取，以此类推。<br>假如当block列表全部读取完成，文件还没读取结束，就调用FileSystem从nn获取下一批次的block列表。</p><p>3.Client调用【FSDataInputStream】对象close方法，关闭输入流。</p>]]></content>
      
      
      
        <tags>
            
            <tag> HDFS读流程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop01单节点部署(基于CDH5.16.2)</title>
      <link href="/article/hadoop01%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/"/>
      <url>/article/hadoop01%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">Hadoop Apache官网</a></p><p>生产上至今企业还是以CDH5.x为主 (cloudera公司)<br>安装版本：hadoop-2.6.0-cdh5.16.2.tar.gz</p><h4 id="hadoop部署">hadoop部署</h4><h5 id="1-hadoop-hdfs部署">1 hadoop hdfs部署</h5><p><strong>部署准备</strong></p><h6 id="11-前面课程已经安装部署jdk">1.1 前面课程已经安装部署jdk</h6><p>jdk需安装(mysql部署时已安装）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 software]$ which java</span><br><span class="line">/usr/java/jdk1.8.0_181/bin/java #CDH版本JDK需安装/usr 目录下不然会有问题</span><br></pre></td></tr></table></figure><p>生产注意点（JDK版本是否兼容）:<br><a href="https://cwiki.apache.org/confluence/display/HADOOP2/HadoopJavaVersions" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/HADOOP2/HadoopJavaVersions</a> --查看apache jdk版本是否兼容<br><a href="https://docs.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_jdk" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_jdk</a> --查看cdh jdk版本是否兼容</p><h6 id="12-创建用户和文件夹">1.2 创建用户和文件夹</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 ~]# useradd hadoop</span><br><span class="line">[root@hadoop01 ~]# su - hadoop</span><br><span class="line">[hadoop@hadoop01 ~]$ mkdir tmp sourcecode software shell log lib  data app</span><br><span class="line">#tmp 存放临时文件如pid</span><br><span class="line">#sourcecode 存放源码包</span><br><span class="line">#software 存放软件压缩包</span><br><span class="line">#shell 存放以后的shell脚本</span><br><span class="line">#lib 存放jar包驱动的</span><br><span class="line">#log 存放日志的</span><br><span class="line">#data 存储第三放数据</span><br><span class="line">#app 存放解压压缩包</span><br><span class="line">[hadoop@hadoop01 ~]$ cd software/</span><br><span class="line">[hadoop@hadoop01 software]$ ll</span><br><span class="line">total 1266604</span><br><span class="line">-rw-r--r-- 1 root   root   434354462 Feb 24 14:01 hadoop-2.6.0-cdh5.16.2.tar.gz</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 185646832 Feb 24 12:03 jdk-8u181-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><h6 id="13-hadoop解压和创建软连接">1.3 hadoop解压和创建软连接</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 software]$ tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/</span><br><span class="line">[hadoop@hadoop01 app]$ </span><br><span class="line">[hadoop@hadoop01 app]$ ll</span><br><span class="line">total 4</span><br><span class="line">drwxr-xr-x 14 hadoop hadoop 4096 Jun  3  2019 hadoop-2.6.0-cdh5.16.2</span><br><span class="line">[hadoop@hadoop01 app]$ ln -s hadoop-2.6.0-cdh5.16.2 hadoop</span><br><span class="line">[hadoop@hadoop01 app]$ ll</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx  1 hadoop hadoop   22 May  6 22:05 hadoop -&gt; hadoop-2.6.0-cdh5.16.2</span><br><span class="line">drwxr-xr-x 14 hadoop hadoop 4096 Jun  3  2019 hadoop-2.6.0-cdh5.16.2</span><br><span class="line">[hadoop@hadoop01 app]$</span><br></pre></td></tr></table></figure><h6 id="14-软连接作用">1.4 软连接作用</h6><p>1）版本切换<br>/home/hadoop/app/hadoop<br>/home/hadoop/app/hadoop-2.6.0-cdh5.16.2<br>想要升级  代码脚本都有仔细检查修改 2–》3<br>如：/home/hadoop/app/hadoop-2.6.0-cdh5.16.3<br>到对应目录:rm -f hadoop<br>重建软连接: ln -s hadoop-2.6.0-cdh5.16.3 hadoop</p><p>但是如果提前设置软连接，代码脚本是hadoop，不关心版本多少</p><p>2）小盘换大盘<br>/根目录磁盘 设置的比较小 20G   /app/log/hadoop-hdfs 文件夹 18G<br>/data01</p><p>mv /app/log/hadoop-hdfs /data01/  ==&gt;/data01/hadoop-hdfs<br>ln -s /data01/hadoop-hdfs /app/log/hadoop-hdfs</p><p>jdk环境变量显性制定下 hadoop app/etc/hadoop</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 app]$ cd hadoop/etc/hadoop</span><br><span class="line">[hadoop@hadoop01 hadoop]$ vi hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line">#显性指定</span><br></pre></td></tr></table></figure><h6 id="15-配置ssh-hadoop01-无密码认证">1.5 配置ssh hadoop01 无密码认证</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 ~]$ rm -rf .ssh  #生产不建议删除容易出生产故障，cd .ssh 查看下是否配置</span><br><span class="line">[hadoop@hadoop01 ~]$ ssh-keygen</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): </span><br><span class="line">Created directory &apos;/home/hadoop/.ssh&apos;.</span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /home/hadoop/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:fhAts9iahMuFy0r/djKCcAO7m8vPm5lf2ExdkWUqIdw hadoop@hadoop01</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|      .... .oo   |</span><br><span class="line">|       ..E..+    |</span><br><span class="line">|        +..o     |</span><br><span class="line">|.    o o.=o      |</span><br><span class="line">| o  o +.S.       |</span><br><span class="line">|o oo ==+ .       |</span><br><span class="line">| +.o=.o+. .      |</span><br><span class="line">|oooo= = ..       |</span><br><span class="line">|++oB+=.+         |</span><br><span class="line">+----[SHA256]-----+</span><br><span class="line">[hadoop@hadoop01 ~]$ cd .ssh </span><br><span class="line">[hadoop@hadoop01 .ssh]$ </span><br><span class="line">[hadoop@hadoop01 .ssh]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">[hadoop@hadoop01 .ssh]$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>–验证ssh是否配置成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 .ssh]$ ssh hadoop01 date</span><br><span class="line">The authenticity of host &apos;hadoop01 (192.168.27.204)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:OLqoaMxlGFbCq4sC9pYgF+FdbcXHbEbtSrnMiGGFbVw.</span><br><span class="line">ECDSA key fingerprint is MD5:d3:5b:4a:ef:8e:00:41:a0:5e:80:ef:75:76:8a:a3:49.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &apos;hadoop01,192.168.27.204&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">Wed May  6 22:26:57 CST 2019</span><br><span class="line">[hadoop@hadoop01 .ssh]$ </span><br><span class="line">[hadoop@hadoop01 .ssh]$ </span><br><span class="line">[hadoop@hadoop01 .ssh]$ ssh hadoop01 date</span><br><span class="line">Wed May  6 22:27:07 CST 2019</span><br><span class="line">[hadoop@hadoop01 .ssh]$</span><br></pre></td></tr></table></figure><h6 id="16-修改配置且hdfs的三个进程都以hadoop01名称启动">1.6 修改配置，且hdfs的三个进程都以hadoop01名称启动</h6><p>修改hdfs三个配置文件<br>1）nn启动以ruozedata名称启动<br>etc/hadoop/core-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop01:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/tmp/&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"># 不要配置在/tmp Linux下tmp目录下会执行30d删除机制</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>2）snn启动以hadoop01名称启动<br>etc/hadoop/hdfs-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop01:9868&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;ruozedata001:9868&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>3）dn启动以ruozedata名称启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 ~]# cd /home/hadoop/app/hadoop/etc/hadoop</span><br><span class="line">[hadoop@hadoop01 hadoop]$ pwd</span><br><span class="line">/home/hadoop/app/hadoop/etc/hadoop</span><br><span class="line">[hadoop@hadoop01 hadoop]$ vi slaves </span><br><span class="line">hadoop01</span><br></pre></td></tr></table></figure><h6 id="17-格式化只需第一次即可格式化自己的编码存储格式">1.7 格式化，只需第一次即可，格式化自己的编码存储格式</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 hadoop]$ pwd</span><br><span class="line">/home/hadoop/app/hadoop</span><br><span class="line">[hadoop@hadoop01 hadoop]$ bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>格式化会生成：</p><ul><li>生成 fsimage</li><li>操作 HDFS 之后生成 edits<br><strong>注意</strong><br>多次格式化之前，需要清空 hadoop.tmp.dir 参数下设置的路径下的所有文件和目 录</li></ul><h6 id="18-启动hdfs">1.8 启动hdfs</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 hadoop]$ sbin/start-dfs.sh</span><br><span class="line">19/10/06 22:43:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Starting namenodes on [hadoop01]</span><br><span class="line">hadoop01: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-namenode-hadoop01.out</span><br><span class="line">hadoop01: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-datanode-hadoop01.out</span><br><span class="line">Starting secondary namenodes [hadoop01]</span><br><span class="line">hadoop01: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-secondarynamenode-hadoop01.out</span><br><span class="line">20/05/06 22:43:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop01 hadoop]$ jps</span><br><span class="line">21712 DataNode    #dn 存储数据的  小弟</span><br><span class="line">21583 NameNode    #nn 负责分配数据存储的 老大</span><br><span class="line">21871 SecondaryNameNode  #snn 万年老二 默认是按1小时粒度去备份老大的数据</span><br><span class="line">21929 Jps</span><br></pre></td></tr></table></figure><h6 id="19-open-web">1.9 open web：</h6><p>通过pid找web端口号<br>netstat nlp | grep 21583     #grep 进程号<br><a href="http://ip" target="_blank" rel="noopener">http://ip</a>:port<br>如：<a href="http://192.168.127.204:50070/dfshealth.html#tab-overview" target="_blank" rel="noopener">http://192.168.127.204:50070/dfshealth.html#tab-overview</a>  #可以打开说明hdfs组件配置成功</p><p><strong>以下测试操作非必须执行步骤</strong></p><h6 id="110-创建文件夹">1.10 创建文件夹</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 hadoop]$  bin/hdfs dfs -mkdir /user</span><br><span class="line">19/10/06 23:03:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop01 hadoop]$ </span><br><span class="line">[hadoop@hadoop01 hadoop]$ </span><br><span class="line">[hadoop@hadoop01 hadoop]$ bin/hdfs dfs -ls /</span><br><span class="line">19/10/06 23:03:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2019-10-06 23:03 /user</span><br><span class="line">#hdfs上的目录</span><br><span class="line">[hadoop@hadoop01 hadoop]$ ls /</span><br><span class="line">bin   dev  home  lib64  meta.js  opt   root  sbin  sys  usr </span><br><span class="line">boot  etc  lib   media  mnt      proc  run   srv   tmp  var   #linux目录</span><br><span class="line">[hadoop@hadoop01 hadoop]$ </span><br><span class="line">[hadoop@hadoop01 hadoop]$ bin/hdfs dfs -mkdir /user/hadoop</span><br></pre></td></tr></table></figure><h6 id="111-上传linuxgthdfs">1.11 上传linux–&gt;hdfs</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 hadoop]$ bin/hdfs dfs -mkdir /wordcount</span><br><span class="line">[hadoop@hadoop01 hadoop]$ bin/hdfs dfs -mkdir /wordcount/input</span><br><span class="line">[hadoop@hadoop01 hadoop]$ bin/hdfs dfs -put  etc/hadoop/*.xml  /wordcount/input/</span><br></pre></td></tr></table></figure><h6 id="112-计算">1.12 计算</h6><p>bin/hadoop jar <br>share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar <br>grep /wordcount/input /wordcount/output ‘dfs[a-z.]+’</p><h6 id="113-下载从hdfsgtlinux">1.13 下载从hdfs–&gt;linux</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop01 hadoop]$ bin/hdfs dfs -get /wordcount/output output</span><br><span class="line">20/05/06 23:13:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop01 hadoop]$ cat output/</span><br><span class="line">cat: output/: Is a directory</span><br><span class="line">[hadoop@hadoop01 hadoop]$ cd output/</span><br><span class="line">[hadoop@hadoop01 output]$ ls</span><br><span class="line">part-r-00000  _SUCCESS</span><br><span class="line">[hadoop@hadoop01 output]$ ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 90 Oct  6 23:13 part-r-00000</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop  0 Oct  6 23:13 _SUCCESS</span><br><span class="line">[hadoop@hadoop01 output]$ cat part-r-00000</span><br><span class="line">1       dfsadmin</span><br><span class="line">1       dfs.replication</span><br><span class="line">1       dfs.namenode.secondary.https</span><br><span class="line">1       dfs.namenode.secondary.http</span><br></pre></td></tr></table></figure><h5 id="2yarn部署">2.YARN部署</h5><p><a href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">参照官方文档</a></p><p>切换用户 su - hadoop #自己用户<br>切换路径  cd /opt/app/hadoop/etc/hadoop/<br>[hadoop@hadoop01 hadoop]$ pwd<br>/home/hadoop/opt/app/hadoop/etc/hadoop/</p><p>修改etc/hadoop/mapred-site.xml:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hadoop]$ su - hadoop</span><br><span class="line">[hadoop@hadoop001 hadoop]$ cd app/hadoop/etc/hadoop/</span><br><span class="line">[hadoop@hadoop001 hadoop]$ cp mapred-site.xml.template mapred-site.xml </span><br><span class="line">#无mapred文件，将.tempele文件拷贝一份改名为mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>etc/hadoop/yarn-site.xml:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop01:18088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>端口号范围：<br>1-65535<br>yarn默认端口号8088<br>但是8088端口暴露外网，会把病毒感染 挖矿程序  计算比特币<br>前面加1即为18088也可改其他数值如7798 范围必须在（1-65535）</p><h6 id="21-启动yarn">2.1 启动yarn</h6><p>Start ResourceManager daemon and NodeManager daemon:<br>[hadoop@hadoop01 hadoop]$ pwd<br>/home/hadoop/app/hadoop/etc/hadoop<br>[hadoop@hadoop01 hadoop]$ cd …/…/<br>[hadoop@hadoop01 hadoop]$ sbin/start-yarn.sh<br>starting yarn daemons<br>starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/yarn-hadoop-resourcemanager-hadoop01.out<br>hadoop01: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/yarn-hadoop-nodemanager-hadoop01.out<br>[hadoop@hadoop01 hadoop]$</p><p>open web: <a href="http://192.168.127.204:18088/cluster" target="_blank" rel="noopener">http://192.168.127.204:18088/cluster</a><br><img src="/img/hadoop_img/yarn_web.png" alt="avatar"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Hadoop01部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL-数据库9道题</title>
      <link href="/article/MySQL%E4%B9%A0%E9%A2%98/"/>
      <url>/article/MySQL%E4%B9%A0%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h4 id="创建表">创建表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">-- 1 部门表</span><br><span class="line">-- dept部门表(deptno部门编号/dname部门名称/loc地点)</span><br><span class="line">drop table IF EXISTS dept;</span><br><span class="line">create table dept (</span><br><span class="line">    deptno numeric(2),</span><br><span class="line">    dname varchar(14),</span><br><span class="line">    loc varchar(13)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">insert into dept values (10, &apos;ACCOUNTING&apos;, &apos;NEW YORK&apos;);</span><br><span class="line">insert into dept values (20, &apos;RESEARCH&apos;, &apos;DALLAS&apos;);</span><br><span class="line">insert into dept values (30, &apos;SALES&apos;, &apos;CHICAGO&apos;);</span><br><span class="line">insert into dept values (40, &apos;OPERATIONS&apos;, &apos;BOSTON&apos;);</span><br><span class="line"></span><br><span class="line">-- 2 工资等级表</span><br><span class="line">-- salgrade工资等级表(grade 等级/losal此等级的最低/hisal此等级的最高)</span><br><span class="line">drop table IF EXISTS salgrade;</span><br><span class="line">create table salgrade (</span><br><span class="line">    grade numeric,</span><br><span class="line">    losal numeric,</span><br><span class="line">    hisal numeric</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">insert into salgrade values (1, 700, 1200);</span><br><span class="line">insert into salgrade values (2, 1201, 1400);</span><br><span class="line">insert into salgrade values (3, 1401, 2000);</span><br><span class="line">insert into salgrade values (4, 2001, 3000);</span><br><span class="line">insert into salgrade values (5, 3001, 9999);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- 3 员工表</span><br><span class="line">-- emp员工表(empno员工号/ename员工姓名/job工作/mgr上级编号</span><br><span class="line">/hiredate受雇日期/sal工资/comm佣金/deptno部门编号)</span><br><span class="line">-- 薪金 ＝ 工资 ＋ 佣金</span><br><span class="line"></span><br><span class="line">drop table IF EXISTS emp;</span><br><span class="line">create table emp (</span><br><span class="line">    empno numeric(4) not null,</span><br><span class="line">    ename varchar(10),</span><br><span class="line">    job varchar(9),</span><br><span class="line">    mgr numeric(4),</span><br><span class="line">    hiredate datetime,</span><br><span class="line">    sal numeric(7, 2),</span><br><span class="line">    comm numeric(7, 2),</span><br><span class="line">    deptno numeric(2)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">insert into emp values (7369, &apos;SMITH&apos;, &apos;CLERK&apos;, 7902, &apos;1980-12-17&apos;, 800, null, 20);</span><br><span class="line">insert into emp values (7499, &apos;ALLEN&apos;, &apos;SALESMAN&apos;, 7698, &apos;1981-02-20&apos;, 1600, 300, 30);</span><br><span class="line">insert into emp values (7521, &apos;WARD&apos;, &apos;SALESMAN&apos;, 7698, &apos;1981-02-22&apos;, 1250, 500, 30);</span><br><span class="line">insert into emp values (7566, &apos;JONES&apos;, &apos;MANAGER&apos;, 7839, &apos;1981-04-02&apos;, 2975, null, 20);</span><br><span class="line">insert into emp values (7654, &apos;MARTIN&apos;, &apos;SALESMAN&apos;, 7698, &apos;1981-09-28&apos;, 1250, 1400, 30);</span><br><span class="line">insert into emp values (7698, &apos;BLAKE&apos;, &apos;MANAGER&apos;, 7839, &apos;1981-05-01&apos;, 2850, null, 30);</span><br><span class="line">insert into emp values (7782, &apos;CLARK&apos;, &apos;MANAGER&apos;, 7839, &apos;1981-06-09&apos;, 2450, null, 10);</span><br><span class="line">insert into emp values (7788, &apos;SCOTT&apos;, &apos;ANALYST&apos;, 7566, &apos;1982-12-09&apos;, 3000, null, 20);</span><br><span class="line">insert into emp values (7839, &apos;KING&apos;, &apos;PRESIDENT&apos;, null, &apos;1981-11-17&apos;, 5000, null, 10);</span><br><span class="line">insert into emp values (7844, &apos;TURNER&apos;, &apos;SALESMAN&apos;, 7698, &apos;1981-09-08&apos;, 1500, 0, 30);</span><br><span class="line">insert into emp values (7876, &apos;ADAMS&apos;, &apos;CLERK&apos;, 7788, &apos;1983-01-12&apos;, 1100, null, 20);</span><br><span class="line">insert into emp values (7900, &apos;JAMES&apos;, &apos;CLERK&apos;, 7698, &apos;1981-12-03&apos;, 950, null, 30);</span><br><span class="line">insert into emp values (7902, &apos;FORD&apos;, &apos;ANALYST&apos;, 7566, &apos;1981-12-03&apos;, 3000, null, 20);</span><br><span class="line">insert into emp values (7934, &apos;MILLER&apos;, &apos;CLERK&apos;, 7782, &apos;1982-01-23&apos;, 1300, null, 10);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from  dept;</span><br><span class="line">select * from  emp;</span><br><span class="line">select * from  salgrade;</span><br></pre></td></tr></table></figure><p><img src="/img/mysql_img/dept.png" alt="avatar"><br><img src="/img/mysql_img/emp.png" alt="avatar"><br><img src="/img/mysql_img/salgrade.png" alt="avatar"></p><h4 id="1-查询出部门编号为30的所有员工的编号和姓名">1. 查询出部门编号为30的所有员工的编号和姓名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select empno,ename from emp where deptno = 30;</span><br></pre></td></tr></table></figure><h4 id="2找出部门编号为10中所有经理和部门编号为20中所有销售员的详细资料">2.找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from emp </span><br><span class="line">where (deptno=10 and job=&apos;manager&apos;) or (deptno=20 and job=&apos;SALESMAN&apos;);</span><br></pre></td></tr></table></figure><h4 id="3查询所有员工详细信息用工资降序排序如果工资相同使用入职日期升序排序">3.查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from emp ORDER BY sal desc,hiredate asc;</span><br></pre></td></tr></table></figure><h4 id="4列出薪金大于1500的各种工作及从事此工作的员工人数">4.列出薪金大于1500的各种工作及从事此工作的员工人数。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select job,count(empno) from </span><br><span class="line">(select job,sal+ifnull(comm,0),empno </span><br><span class="line">from emp </span><br><span class="line">where sal+ifnull(comm,0)&gt;1500) as empnew  GROUP BY job;</span><br></pre></td></tr></table></figure><h4 id="5列出在销售部工作的员工的姓名假定不知道销售部的部门编号">5.列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</h4><p>解一：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select e.ename</span><br><span class="line">from dept as d</span><br><span class="line">INNER JOIN emp as e on d.deptno=e.deptno where d.dname=&apos;SALES&apos;;</span><br></pre></td></tr></table></figure><p>解二(子查询):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select e.ename </span><br><span class="line">from emp e where e.deptno=(select deptno from deptwhere dname=&apos;SALES&apos;);</span><br></pre></td></tr></table></figure><h4 id="6查询姓名以s开头的以s结尾包含s字符第二个字母为l">6.查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L</h4><p>合并写</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from emp</span><br><span class="line">where ename like &apos;S%&apos; or ename like &apos;%S&apos; or ename like &apos;%S%&apos; or ename like &apos;_L%&apos;;</span><br></pre></td></tr></table></figure><h4 id="7查询每种工作的最高工资-最低工资-人数">7.查询每种工作的最高工资、最低工资、人数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELECT job,MAX(SAL),MIN(SAL),count(empno) </span><br><span class="line">from emp GROUP BY job;</span><br></pre></td></tr></table></figure><h4 id="8列出薪金-高于-公司平均薪金的所有员工号员工姓名所在部门名称上级领导工资工资等级">8.列出薪金 高于 公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT e.empno 员工号,e.ename 员工姓名,d.dname 部门名称,m.ename 上级领导,e.sal 工资,s.grade 工资等级</span><br><span class="line">from emp e </span><br><span class="line">LEFT JOIN dept d on e.deptno=d.deptno</span><br><span class="line">LEFT JOIN emp m on e.mgr=m.empno</span><br><span class="line">LEFT JOIN salgrade s on e.sal BETWEEN losal and hisal </span><br><span class="line">where (e.sal+ifnull(e.comm,0)) &gt; (select avg(e.sal+ifnull(e.comm,0)) from emp e);</span><br></pre></td></tr></table></figure><p><strong>注意用自连接如：</strong><br>select m.ename 上级领导 from emp e join emp m on e.mgr=m.empno;</p><h4 id="9列出薪金-高于-在部门30工作的-所有任何一个员工的薪金的员工姓名和薪金-部门名称">9.列出薪金  高于  在部门30工作的  所有/任何一个员工的薪金的员工姓名和薪金、部门名称。</h4><p>解一(视图所有):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">drop view en;</span><br><span class="line">create view en </span><br><span class="line">as</span><br><span class="line">select ename,sal+IFNULL(comm,0) as esal,dname,dept.deptno </span><br><span class="line">from emp </span><br><span class="line">join dept on emp.deptno=dept.deptno;</span><br><span class="line"></span><br><span class="line">select * from en where esal&gt;(select max(esal) from en where deptno=30);</span><br></pre></td></tr></table></figure><p>解一(所有):</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select * from (select ename,sal+IFNULL(comm,0) as esal,dname,dept.deptno from emp </span><br><span class="line">join dept on emp.deptno=dept.deptno) sn </span><br><span class="line">where sn.esal &gt;</span><br><span class="line">(select max(esal) from </span><br><span class="line">(select ename,sal+IFNULL(comm,0) as esal,deptno from emp ) as de </span><br><span class="line">where de.deptno=30);  </span><br><span class="line">-- 高于  在部门30工作的  所有等同于高于30部门最高工资</span><br></pre></td></tr></table></figure><p>解二(所有):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select * from (select ename,sal+IFNULL(comm,0) as esal,dname,dept.deptno from emp </span><br><span class="line">join dept on emp.deptno=dept.deptno) sn where sn.esal &gt;</span><br><span class="line">ALL(select esal from </span><br><span class="line">(select ename,sal+IFNULL(comm,0) as esal,deptno from emp ) as de </span><br><span class="line">where de.deptno=30);</span><br></pre></td></tr></table></figure><p>解一(任何):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select * from (select ename,sal+IFNULL(comm,0) as esal,dname,dept.deptno from emp </span><br><span class="line">join dept on emp.deptno=dept.deptno) sn where sn.esal &gt;</span><br><span class="line">(select min(esal) from </span><br><span class="line">(select ename,sal+IFNULL(comm,0) as esal,deptno from emp ) as de </span><br><span class="line">where de.deptno=30); -- 高于  在部门30工作的  任何等同于高于30部门最低工资</span><br></pre></td></tr></table></figure><p>解二(任何):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select * from (select ename,sal+IFNULL(comm,0) as esal,dname,dept.deptno from emp </span><br><span class="line">join dept on emp.deptno=dept.deptno) sn where sn.esal &gt;</span><br><span class="line">ANY(select esal from </span><br><span class="line">(select ename,sal+IFNULL(comm,0) as esal,deptno from emp ) as de </span><br><span class="line">where de.deptno=30);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> MySQL-execises </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL01(安装部署)</title>
      <link href="/article/MySQL01/"/>
      <url>/article/MySQL01/</url>
      
        <content type="html"><![CDATA[<h4 id="一mysql安装部署">一.mysql安装部署</h4><p>安装准备下载JDK及mysql的安装包<br>JDK版本：jdk-8u181-linux-x64.tar.gz<br>JDK下载地址:<a href="https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html" target="_blank" rel="noopener">https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html</a><br>mysql版本：mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz<br>mysql下载地址:<a href="https://downloads.mysql.com/archives/community/" target="_blank" rel="noopener">https://downloads.mysql.com/archives/community/</a></p><h5 id="1上传jdk及mysql安装包">1.上传JDK及mysql安装包</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 ~]# ll -h</span><br><span class="line">-rw-r--r-- 1 root root 166M Jul 24  2018 jdk-8u181-linux-x64.gz</span><br><span class="line">-rw-r--r-- 1 root root 523M Jul 24  2018 mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz</span><br></pre></td></tr></table></figure><h5 id="2jdk部署">2.JDK部署</h5><p>jdk-8u181-linux-x64.tar.gz</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 ~]# mkdir  /usr/java</span><br><span class="line">[root@hadoop01 ~]# tar -xzvf jdk-8u118-linux-x64.gz -C /usr/java/</span><br><span class="line">[root@hadoop01 ~]# cd     /usr/java</span><br><span class="line">[root@hadoop01 ~]# chown -R root:root jdk1.8.0_181</span><br><span class="line"></span><br><span class="line">[root@hadoop01 ~]# vi /etc/profile</span><br><span class="line">#learn env</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">[root@hadoop01 ~]# source /etc/profile</span><br><span class="line">[root@hadoop01 ~]# which java</span><br><span class="line">/usr/java/jdk1.8.0_181/bin/java</span><br></pre></td></tr></table></figure><p><strong>注意：</strong><br>(1) mkdir  /usr/java–&gt; cd /usr/java–&gt; tar -xzvf jdk-8u45-linux-x64.gz -C /usr/java/  (要安装CDH版本要放/usr/下不然有问题）<br>(2) 注意权限！cd   /usr/java–&gt;chown -R root:root jdk1.8.0_181</p><h5 id="3mysql部署-5711">3.MySQL部署 5.7.11</h5><p>移动安装包放置对应目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 ~]# mv  mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz  /usr/local</span><br><span class="line">[root@hadoop01 ~]# cd /usr/local</span><br></pre></td></tr></table></figure><h6 id="31解压及创建目录">3.1解压及创建目录</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 local]# tar xzvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz</span><br><span class="line">[root@hadoop01 local]# mv mysql-5.7.11-linux-glibc2.5-x86_64 mysql </span><br><span class="line">[root@hadoop01 local]# mkdir mysql/arch mysql/data mysql/tmp</span><br></pre></td></tr></table></figure><h6 id="32创建mycnf">3.2创建my.cnf</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 local]# vi /etc/my.cnf （清空拷贝以下）</span><br><span class="line">[client]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/local/mysql/data/mysql.sock</span><br><span class="line">default-character-set=utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/local/mysql/data/mysql.sock</span><br><span class="line"></span><br><span class="line">skip-slave-start</span><br><span class="line"></span><br><span class="line">skip-external-locking</span><br><span class="line">key_buffer_size = 256M</span><br><span class="line">sort_buffer_size = 2M</span><br><span class="line">read_buffer_size = 2M</span><br><span class="line">read_rnd_buffer_size = 4M</span><br><span class="line">query_cache_size= 32M</span><br><span class="line">max_allowed_packet = 16M</span><br><span class="line">myisam_sort_buffer_size=128M</span><br><span class="line">tmp_table_size=32M</span><br><span class="line"></span><br><span class="line">table_open_cache = 512</span><br><span class="line">thread_cache_size = 8</span><br><span class="line">wait_timeout = 86400</span><br><span class="line">interactive_timeout = 86400</span><br><span class="line">max_connections = 600</span><br><span class="line"></span><br><span class="line"># Try number of CPU&apos;s*2 for thread_concurrency</span><br><span class="line">#thread_concurrency = 32 </span><br><span class="line"></span><br><span class="line">#isolation level and default engine </span><br><span class="line">default-storage-engine = INNODB</span><br><span class="line">transaction-isolation = READ-COMMITTED</span><br><span class="line"></span><br><span class="line">server-id  = 1739</span><br><span class="line">basedir     = /usr/local/mysql</span><br><span class="line">datadir     = /usr/local/mysql/data</span><br><span class="line">pid-file     = /usr/local/mysql/data/hostname.pid</span><br><span class="line"></span><br><span class="line">#open performance schema</span><br><span class="line">log-warnings</span><br><span class="line">sysdate-is-now</span><br><span class="line"></span><br><span class="line">binlog_format = ROW</span><br><span class="line">log_bin_trust_function_creators=1</span><br><span class="line">log-error  = /usr/local/mysql/data/hostname.err</span><br><span class="line">log-bin = /usr/local/mysql/arch/mysql-bin</span><br><span class="line">expire_logs_days = 7</span><br><span class="line"></span><br><span class="line">innodb_write_io_threads=16</span><br><span class="line"></span><br><span class="line">relay-log  = /usr/local/mysql/relay_log/relay-log</span><br><span class="line">relay-log-index = /usr/local/mysql/relay_log/relay-log.index</span><br><span class="line">relay_log_info_file= /usr/local/mysql/relay_log/relay-log.info</span><br><span class="line"></span><br><span class="line">log_slave_updates=1</span><br><span class="line">gtid_mode=OFF</span><br><span class="line">enforce_gtid_consistency=OFF</span><br><span class="line"></span><br><span class="line"># slave</span><br><span class="line">slave-parallel-type=LOGICAL_CLOCK</span><br><span class="line">slave-parallel-workers=4</span><br><span class="line">master_info_repository=TABLE</span><br><span class="line">relay_log_info_repository=TABLE</span><br><span class="line">relay_log_recovery=ON</span><br><span class="line"></span><br><span class="line">#other logs</span><br><span class="line">#general_log =1</span><br><span class="line">#general_log_file  = /usr/local/mysql/data/general_log.err</span><br><span class="line">#slow_query_log=1</span><br><span class="line">#slow_query_log_file=/usr/local/mysql/data/slow_log.err</span><br><span class="line"></span><br><span class="line">#for replication slave</span><br><span class="line">sync_binlog = 500</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#for innodb options </span><br><span class="line">innodb_data_home_dir = /usr/local/mysql/data/</span><br><span class="line">innodb_data_file_path = ibdata1:1G;ibdata2:1G:autoextend</span><br><span class="line"></span><br><span class="line">innodb_log_group_home_dir = /usr/local/mysql/arch</span><br><span class="line">innodb_log_files_in_group = 4</span><br><span class="line">innodb_log_file_size = 1G</span><br><span class="line">innodb_log_buffer_size = 200M</span><br><span class="line"></span><br><span class="line">#根据生产需要，调整pool size </span><br><span class="line">innodb_buffer_pool_size = 2G</span><br><span class="line">#innodb_additional_mem_pool_size = 50M #deprecated in 5.6</span><br><span class="line">tmpdir = /usr/local/mysql/tmp</span><br><span class="line"></span><br><span class="line">innodb_lock_wait_timeout = 1000</span><br><span class="line">#innodb_thread_concurrency = 0</span><br><span class="line">innodb_flush_log_at_trx_commit = 2</span><br><span class="line"></span><br><span class="line">innodb_locks_unsafe_for_binlog=1</span><br><span class="line"></span><br><span class="line">#innodb io features: add for mysql5.5.8</span><br><span class="line">performance_schema</span><br><span class="line">innodb_read_io_threads=4</span><br><span class="line">innodb-write-io-threads=4</span><br><span class="line">innodb-io-capacity=200</span><br><span class="line">#purge threads change default(0) to 1 for purge</span><br><span class="line">innodb_purge_threads=1</span><br><span class="line">innodb_use_native_aio=on</span><br><span class="line"></span><br><span class="line">#case-sensitive file names and separate tablespace</span><br><span class="line">innodb_file_per_table = 1</span><br><span class="line">lower_case_table_names=1</span><br><span class="line"></span><br><span class="line">[mysqldump]</span><br><span class="line">quick</span><br><span class="line">max_allowed_packet = 128M</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">no-auto-rehash</span><br><span class="line">default-character-set=utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqlhotcopy]</span><br><span class="line">interactive-timeout</span><br><span class="line"></span><br><span class="line">[myisamchk]</span><br><span class="line">key_buffer_size = 256M</span><br><span class="line">sort_buffer_size = 256M</span><br><span class="line">read_buffer = 2M</span><br><span class="line">write_buffer = 2M</span><br></pre></td></tr></table></figure><h6 id="33-创建用户组及用户">3.3 创建用户组及用户</h6><p>  创建用户mysqladmin useradd mysqladmin</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 local]# groupadd -g 101 dba</span><br><span class="line">[root@hadoop01 local]# useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin</span><br><span class="line">[root@hadoop01 local]# id mysqladmin</span><br><span class="line">uid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root)</span><br></pre></td></tr></table></figure><p>  创建用户组及用户方便管理<br>  mysql  mysql用户来管理<br>  oracle oracle用户<br>  hadoop hive   bigdata用户来管理</p><h6 id="34-copy环境变量配置文件至mysqladmin用户的home目录中为了以下步骤配置个人环境变量重要">3.4 copy环境变量配置文件至mysqladmin用户的home目录中,为了以下步骤配置个人环境变量(重要）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 local]# cp /etc/skel/.* /usr/local/mysql  ###important</span><br></pre></td></tr></table></figure><h6 id="35-配置环境变量">3.5 配置环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 local]# vi mysql/.bashrc</span><br><span class="line"># .bash_profile</span><br><span class="line"># Get the aliases and functions</span><br><span class="line">if [ -f ~/.bashrc ]; then</span><br><span class="line">        . ~/.bashrc</span><br><span class="line">fi</span><br><span class="line"># User specific environment and startup programs</span><br><span class="line">export MYSQL_BASE=/usr/local/mysql</span><br><span class="line">export PATH=$&#123;MYSQL_BASE&#125;/bin:$PATH</span><br><span class="line"></span><br><span class="line">unset USERNAME</span><br><span class="line"></span><br><span class="line">#stty erase ^H</span><br><span class="line">set umask to 022</span><br><span class="line">umask 022</span><br><span class="line">PS1=`uname -n`&quot;:&quot;&apos;$USER&apos;&quot;:&quot;&apos;$PWD&apos;&quot;:&gt;&quot;; export PS1</span><br><span class="line"></span><br><span class="line">## end</span><br></pre></td></tr></table></figure><h6 id="36-赋权限和用户组切换用户mysqladmin安装">3.6 赋权限和用户组，切换用户mysqladmin，安装</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 local]# chown  mysqladmin:dba /etc/my.cnf </span><br><span class="line">[root@hadoop01 local]# chmod  640 /etc/my.cnf  </span><br><span class="line">[root@hadoop01 local]# chown -R mysqladmin:dba /usr/local/mysql</span><br><span class="line">[root@hadoop01 local]# chmod -R 755 /usr/local/mysql</span><br></pre></td></tr></table></figure><h6 id="37-配置服务及开机自启动可省略此步骤但需要启动需要配置">3.7 配置服务及开机自启动(可省略此步骤但需要启动需要配置）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 local]# cd /usr/local/mysql</span><br><span class="line">#将服务文件拷贝到init.d下，并重命名为mysql</span><br><span class="line">[root@hadoop01 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql </span><br><span class="line">#赋予可执行权限</span><br><span class="line">[root@hadoop01 mysql]# chmod +x /etc/rc.d/init.d/mysql</span><br><span class="line">#删除服务</span><br><span class="line">[root@hadoop01 mysql]# chkconfig --del mysql</span><br><span class="line">#添加服务</span><br><span class="line">[root@hadoop01 mysql]# chkconfig --add mysql</span><br><span class="line">[root@hadoop01 mysql]# chkconfig --level 345 mysql on</span><br><span class="line">[root@hadoop01 mysql]# vi /etc/rc.local</span><br><span class="line">#!/bin/sh</span><br><span class="line">#</span><br><span class="line"># This script will be executed *after* all the other init scripts.</span><br><span class="line"># You can put your own initialization stuff in here if you don&apos;t</span><br><span class="line"># want to do the full Sys V style init stuff.</span><br><span class="line"></span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line"></span><br><span class="line">su - mysqladmin -c &quot;/etc/init.d/mysql start --federated&quot;</span><br><span class="line">&quot;/etc/rc.local&quot; 9L, 278C written</span><br></pre></td></tr></table></figure><h6 id="38-安装libaio及安装mysql的初始db">3.8 安装libaio及安装mysql的初始db</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 mysql]# yum -y install libaio</span><br><span class="line">[root@hadoop01 mysql]# sudo su - mysqladmin</span><br><span class="line">hadoop01:mysqladmin:/usr/local/mysql:&gt; bin/mysqld \</span><br><span class="line">--defaults-file=/etc/my.cnf \</span><br><span class="line">--user=mysqladmin \</span><br><span class="line">--basedir=/usr/local/mysql/ \</span><br><span class="line">--datadir=/usr/local/mysql/data/ \</span><br><span class="line">--initialize</span><br></pre></td></tr></table></figure><h6 id="39-查看临时密码">3.9 查看临时密码</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop01:mysqladmin:/usr/local/mysql:&gt; cd data</span><br><span class="line">hadoop01:mysqladmin:/usr/local/mysql/data:&gt;cat hostname.err |grep password </span><br><span class="line">2018-05-10T02:15:29.439671Z 1 [Note] A temporary password is generated for root@localhost: kFCqrXeh2y(0</span><br></pre></td></tr></table></figure><h6 id="310-启动">3.10 启动</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 mysql]# su  mysqladmin</span><br><span class="line">hadoop01:mysqladmin:/usr/local/mysql:&gt;/usr/local/mysql/bin/mysqld_safe --defaults-file=/etc/my.cnf &amp;</span><br></pre></td></tr></table></figure><p>回车后光标跳动继续回车一次</p><h6 id="311-登录及修改用户密码">3.11 登录及修改用户密码</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hadoop01:mysqladmin:/usr/local/mysql/data:&gt;cat hostname.err |grep password(密码）</span><br><span class="line">hadoop01:mysqladmin:/usr/local/mysql/data:&gt;mysql -uroot -p&apos;密码&apos;</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.7.11-log</span><br><span class="line">..........</span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line">mysql&gt; alter user root@localhost identified by &apos;data123456&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.05 sec)</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;data123456&apos; ;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.02 sec)</span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line">mysql&gt; exit;</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure><h6 id="312-重启">3.12 重启</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop01:mysqladmin:/usr/local/mysql:&gt; service mysql restart</span><br></pre></td></tr></table></figure><h4 id="二重新部署怎么修改">二.重新部署怎么修改</h4><h5 id="1-删除或移走最好移走">1 删除或移走（最好移走）</h5><p>rm -rf mysql/arch/*  binlog文件夹(或移走）<br>rm -rf mysql/data/*  数据文件(或移走）</p><h5 id="2-初始化">2 初始化</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/mysqld \</span><br><span class="line">--defaults-file=/etc/my.cnf \</span><br><span class="line">--user=mysqladmin \</span><br><span class="line">--basedir=/usr/local/mysql/ \</span><br><span class="line">--datadir=/usr/local/mysql/data/ \</span><br><span class="line">--initialize</span><br></pre></td></tr></table></figure><h4 id="三安装dbeaver-海狸测试连接mysql">三.安装dbeaver 海狸测试连接mysql</h4><h5 id="1-window-mac-安装好jdk18">1 window / mac 安装好jdk1.8</h5><p><a href="https://jingyan.baidu.com/article/b907e62789c79346e7891ced.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/b907e62789c79346e7891ced.html</a></p><h5 id="2-下载-绿色版本">2 下载 绿色版本</h5><p><a href="https://dbeaver.io/files/dbeaver-ce-latest-win32.win32.x86_64.zip" target="_blank" rel="noopener">https://dbeaver.io/files/dbeaver-ce-latest-win32.win32.x86_64.zip</a></p><h4 id="四常用命令">四.常用命令</h4><p>  create database localdata;<br>  grant all privileges on localdata.* to hadoop01@’%’ identified by ‘data123456’;#data123456密码自设<br>  flush privileges;  养成习惯！！！<br>  % 代表任意客户端机器去访问mysql服务器<br>  192.168.1.1  这台机器有权限去访问<br>  192.168.1.%  这个网段的所有ip机器有权限去访问<br>  检验:<br>  select user,authentication_string,host from mysql.user;</p><h4 id="五登录">五.登录</h4><p>mysql -uroot -pdata123456 -hhadoop01 -P3306 mysql<br>Usage: mysql [OPTIONS] [database]<br>database schema namespace 是一个意思  就可以认为是一个文件夹<br>table    就可以认为是一个Excel表格<br>切换：<br>mysql&gt; use localdata;<br>Database changed<br>mysql&gt;<br>mysql&gt; show tables;<br>Empty set (0.00 sec)<br><strong><font color="green"> 提醒:</font></strong> 密码不要写在命令行 不然 history命令可以发现<br>mysql -uroot -p -hhadoop01 -P3306  直接回车</p><p><strong><font color="green"> 注意</font></strong><br>mysql -u root -p data123456 -h hadoop01 -P 3306 mysql<br>-p后面如果非要写密码 不能有空格，其他参数无所谓！</p><h4 id="六进程">六.进程</h4><h5 id="1-查看进程及杀死进程">1 查看进程及杀死进程</h5><p>show processlist;<br>kill Id</p>]]></content>
      
      
      
        <tags>
            
            <tag> MySQL01 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux04命令</title>
      <link href="/article/linux04%E5%91%BD%E4%BB%A4/"/>
      <url>/article/linux04%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h4 id="1系统常用检查命令">1.系统常用检查命令</h4><p>(1) 磁盘 df -h</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[jacket@learn01 ~]$ df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        40G   16G   25G  39% /</span><br><span class="line"></span><br><span class="line">/dev/vdb1        2T   16G   25G  1% /data01</span><br><span class="line">/dev/vdb2        2T   16G   25G  1% /data02</span><br><span class="line">/dev/vdb3        2T   16G   25G  1% /data03</span><br><span class="line">/dev/vdb4        2T   16G   25G  1% /data04</span><br></pre></td></tr></table></figure><p>500G ssd<br>不要</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">devtmpfs        3.9G     0  3.9G   0% /dev</span><br><span class="line">tmpfs           3.9G   16K  3.9G   1% /dev/shm</span><br><span class="line">tmpfs           3.9G  258M  3.6G   7% /run</span><br><span class="line">tmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup</span><br><span class="line">tmpfs           783M     0  783M   0% /run/user/1004</span><br></pre></td></tr></table></figure><p>(2) 内存 free -m</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[jacket@learn01 ~]$ free -m </span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           7823         222        6229         257        1371        7096</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure><p>大数据 生产服务器 swap是设置0  10也可以</p><p>(3) 负载 top<br>系统负载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">top - 21:20:22 up 7 days, 58 min,  1 user,  load average: 0.01, 0.03, 0.05</span><br><span class="line">Tasks:  89 total,   1 running,  88 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.2 us,  0.5 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.3 st</span><br><span class="line">KiB Mem :  8011076 total,  6377388 free,   229060 used,  1404628 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.  7265724 avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                  </span><br><span class="line"> 2374 root      20   0  394348  31376   8608 S   0.3  0.4  41:44.99 jdog-kunlunmirr                          </span><br><span class="line">    1 root      20   0  125356   3796   2508 S   0.0  0.0   1:22.32 systemd                                  </span><br><span class="line">    2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd                                 </span><br><span class="line">    3 root      20   0       0      0      0 S   0.0  0.0   0:00.08 ksoftirqd/0                              </span><br><span class="line">    5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H                             </span><br><span class="line">    6 root      20   0       0      0      0 S   0.0  0.0   0:02.50 kworker/u4:0</span><br></pre></td></tr></table></figure><p>load average: 0.01, 0.03, 0.05<br>      1min   5min  15min</p><p>经验值: 10  生产不用超过这个 ，否则认为服务器就是卡<br>  a.是你的程序有问题 在大量跑计算<br>  b.是不是被挖矿 yarn redis 最容易被hacker 攻击<br>  c.硬件问题  内存条 硬盘   重启</p><h4 id="2yum安装卸载">2.yum安装卸载</h4><p>(1) yum search httpd(搜索)<br>  yum install httpd(安装)</p><p>  centos6:<br>  service httpd status|start|stop  1个应用httpd</p><p>  centos7:<br>  service httpd status|start|stop  兼容<br>  systemctl status|start|stop httpd app2 app3 app4  一次性操作多个应用</p><p>(2)搜索 卸载:rpm -qa|grep http<br>  卸载方法一：rpm -e 包名称 --nodeps<br>  卸载方法二：yum remove httpd-2.4.6-90.el7.centos.x86_64<br>搜索：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> [root@learn01 ~]# rpm -qa|grep http</span><br><span class="line">httpd-2.4.6-90.el7.centos.x86_64</span><br><span class="line">httpd-tools-2.4.6-90.el7.centos.x86_64</span><br></pre></td></tr></table></figure><p>卸载：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# rpm -e 包名称 --nodeps</span><br><span class="line">[root@learn01 ~]# yum remove httpd-2.4.6-90.el7.centos.x86_64</span><br></pre></td></tr></table></figure><h4 id="3进程-端口号">3.进程 端口号</h4><p>(1) ps -ef | grep http(过滤查看对应进程）<br>杀死进程：<br>  kill -9 16629<br>  kill -9 16630 16631  16632  16633  16634（杀死多个进程）<br>  kill -9 $(pgrep -f httpd)  <font color="red">  杀死httpd全部进程（慎用）</font><br>根据匹配字段 搜索所有符合的进程 全部杀死<br>但是: 生产慎用 除非你先ps查看 这个关键词搜索的进程 是不是都是你想要杀死的进程<br>保不齐有个其他服务的进程 会造成误杀 生产事故！！！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# ps -ef|grep http</span><br><span class="line">root     18363     1  0 21:51 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18364 18363  0 21:51 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18365 18363  0 21:51 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18366 18363  0 21:51 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18367 18363  0 21:51 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18368 18363  0 21:51 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">root     18387 15881  0 21:51 pts/2    00:00:00 grep --color=auto http</span><br><span class="line">[root@learn01 ~]# kill -9 $(pgrep -f httpd)</span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# ps -ef|grep http</span><br><span class="line">root     18444 15881  0 21:52 pts/2    00:00:00 grep --color=auto http</span><br></pre></td></tr></table></figure><p>(2) netstat -nlp| grep 进程号(过滤查看对应端口号）<br>进程不一定都会起到端口号<br>但是  与其他服务通信 比如需要端口号！！！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# ps -ef|grep http</span><br><span class="line">root     18670     1  0 21:53 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18671 18670  0 21:53 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18672 18670  0 21:53 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18673 18670  0 21:53 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18674 18670  0 21:53 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">apache   18675 18670  0 21:53 ?        00:00:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">root     18696 15881  0 21:54 pts/2    00:00:00 grep --color=auto http</span><br><span class="line"></span><br><span class="line">[root@learn01 ~]# yum install -y net-tools.x86_64</span><br><span class="line">[root@learn01 ~]# netstat -nlp| grep 18670</span><br><span class="line">tcp6       0      0 :::80                   :::*                    LISTEN      18670/httpd         </span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# netstat -nlp| grep 18671</span><br><span class="line">[root@learn01 ~]# netstat -nlp| grep 18672</span><br><span class="line">[root@learn01 ~]# netstat -nlp| grep 18673</span><br><span class="line">[root@learn01 ~]#</span><br></pre></td></tr></table></figure><p><font color="green"> (思考) </font>老板: 去打开xxx服务器的应用yyy的网页？你会涉及到哪些Linux命令<br>ip<br>ps -ef|grep yyy --》pid<br>netstat -nlp|grep pid --》port</p><blockquote><p>浏览器: <a href="http://ip" target="_blank" rel="noopener">http://ip</a>:port</p></blockquote><p><strong>细节:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# netstat -nlp| grep 18670</span><br><span class="line">tcp6       0      0 :::80                   :::*                    LISTEN      18670/httpd         </span><br><span class="line">tcp6       0      0 0.0.0.0:80                   :::*                    LISTEN      18670/httpd         </span><br><span class="line">tcp6       0      0 192.168.0.3:80                   :::*                    LISTEN      18670/httpd</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tcp6       0      0 127.0.0.1:80                   :::*                    LISTEN      18670/httpd         </span><br><span class="line">tcp6       0      0 localhost:80                   :::*                    LISTEN      18670/httpd         </span><br><span class="line">危险: 该服务只能自己服务器的里面自己访问自己</span><br></pre></td></tr></table></figure><p>(3) telnet ip port</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# learn01 809</span><br><span class="line">Trying 192.168.0.3...</span><br><span class="line">telnet: connect to address 192.168.0.3: Connection refused  服务器拒绝连接Connection refused</span><br></pre></td></tr></table></figure><p>有可能你的服务器 防火墙 开启，云主机 需要开启 安全组策略<br>直接找Linux运维 网络工程师 加防火墙(硬件)策略</p><p>总结:<br>Connection refused<br>1.ping   ip 因为服务器是ping功能禁止<br>2.telnet ip  port  ok</p><p>配置企业级别yum源 取代互联网的repo文件的URL<br>物理隔绝</p><h4 id="4下载">4.下载</h4><p>方法一：wget <a href="https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar" target="_blank" rel="noopener">https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar</a><br>方法二：curl <a href="https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar" target="_blank" rel="noopener">https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar</a>  -O  spark-core_2.12-2.4.5.jar</p><h4 id="5压缩-解压">5.压缩 解压</h4><p>（1）zip<br>  zip压缩：zip -r xxx.zip xxx/*<br>  zip解压缩：unzip xxx.zip<br>（2）.tar.gz<br>  压缩：tar -czvf xxxx.tar.gz  xxxx/*<br>  解压缩：tar -xzvf xxxx.tar.gz</p><h4 id="6命令找不到">6.命令找不到</h4><p>which 命令<br>which xxx<br>想要命令快速找到  which xxx 来验证，<br>其实就是提前将命令的目录配置在环境变量$PATH<br>echo $PATH 来查看是否将命令的目录配置上！</p><h4 id="7定时">7.定时</h4><p>(1)crontab -l (list)#列出计划任务<br>(2)crontab -r(remove) #删除计划任务<br>(3)crontab -e (edit)#编辑周期性计划任务<br>*    *     *    *    *   user-name command to be executed<br>分  时  日  月  周  命令</p><p>脚本:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# vi base.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line">date</span><br><span class="line"></span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# ll</span><br><span class="line">total 8</span><br><span class="line">drwxr-xr-x 2 root root  30 Apr 22 22:50 dir1</span><br><span class="line">-rw-r--r-- 1 root root 128 Apr 22 22:49 dir1.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root  19 Apr 22 23:02 base.sh</span><br><span class="line">[root@learn01 ~]# ./base.sh</span><br><span class="line">-bash: ./base.sh: Permission denied</span><br><span class="line">[root@learn01 ~]# sh ./base.sh</span><br><span class="line">Wed Apr 22 23:02:36 CST 2018</span><br><span class="line">[root@learn01 ~]# chmod 744 base.sh</span><br><span class="line">[root@learn01 ~]# ll</span><br><span class="line">total 8</span><br><span class="line">drwxr-xr-x 2 root root  30 Apr 22 22:50 dir1</span><br><span class="line">-rw-r--r-- 1 root root 128 Apr 22 22:49 dir1.tar.gz</span><br><span class="line">-rwxr--r-- 1 root root  19 Apr 22 23:02 base.sh</span><br><span class="line">[root@learn01 ~]# ./base.sh </span><br><span class="line">Wed Apr 22 23:03:17 CST 2018</span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# date</span><br><span class="line">Wed Apr 22 23:03:21 CST 2018</span><br><span class="line">[root@learn01 ~]# crontab -e</span><br><span class="line">* * * * * /root/base.sh &gt;&gt; /root/base.log</span><br></pre></td></tr></table></figure><p><font color="green"> (思考) </font>eg: 每隔10s打印一次 怎么做</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi test1.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line">for((i=1;i&lt;=6;i++));</span><br><span class="line">do</span><br><span class="line">date</span><br><span class="line">sleep 10s</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h4 id="8后台执行脚本">8.后台执行脚本</h4><p>nohup  /root/test.sh &gt;&gt; /root/test.log 2&gt;&amp;1 &amp;  生产标准写法</p><blockquote><p>nohup :后台托管表示永久执行命令，哪怕当前终端已经退出登录<br>&amp;：后台执行命令。<br>2&gt;&amp;1：<br>在bash shell中，<br>0代表标准输入，一般是键盘录入；<br>1代表标准输出,一般是屏幕；<br>2代表标准错误；<br>因此当命令使用nohup &amp;运行以后，标准都错误都输出到2去了，console上看不到输出的错误。<br>2&gt;&amp;1，起到了一个重定向都作用，将标准错误重定向到标准输出上去，后台运行的程序就可以在&gt;屏幕上看到程序输出的错误了。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Linux04 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux03命令</title>
      <link href="/article/linux03%E5%91%BD%E4%BB%A4/"/>
      <url>/article/linux03%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h4 id="1查看用户-用户组">1.查看用户 用户组</h4><p>  (1)用户：ll /usr/sbin/user*<br>  (2)用户组：ll /usr/sbin/group*<br>  用户存储信息 /etc/passwd<br>  用户组       /etc/group</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# ll /usr/sbin/user*</span><br><span class="line">-rwxr-x--- 1 root root 118192 Nov  6  2016 /usr/sbin/useradd</span><br><span class="line">-rwxr-x--- 1 root root  80360 Nov  6  2016 /usr/sbin/userdel</span><br><span class="line">-rwxr-x--- 1 root root 113840 Nov  6  2016 /usr/sbin/usermod</span><br><span class="line">-rwsr-xr-x 1 root root  11296 Apr 13  2017 /usr/sbin/usernetctl</span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# ll /usr/sbin/group*</span><br><span class="line">-rwxr-x--- 1 root root 65480 Nov  6  2016 /usr/sbin/groupadd</span><br><span class="line">-rwxr-x--- 1 root root 57016 Nov  6  2016 /usr/sbin/groupdel</span><br><span class="line">-rwxr-x--- 1 root root 57064 Nov  6  2016 /usr/sbin/groupmems</span><br><span class="line">-rwxr-x--- 1 root root 76424 Nov  6  2016 /usr/sbin/groupmod</span><br></pre></td></tr></table></figure><h4 id="2创建-删除用户-用户组">2.创建、删除用户 用户组</h4><p>(1) 创建用户：useradd 用户名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# useradd jacket</span><br><span class="line">[root@learn01 ~]# id jacket</span><br><span class="line">uid=1004(jacket) gid=1004(jacket) groups=1004(jacket)</span><br><span class="line">[root@learn01 ~]#</span><br></pre></td></tr></table></figure><p>id 用户名(jacket) 创建一个普通用户  用户名称 jacket<br>同时也会创建一个 jacket用户组<br>设置jacket用户的组为jacket,且把这个jacket用户组设置为 主组<br>同时也创建家目录  /home/jacket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# ll /home</span><br><span class="line">drwx------   2 jacket  jacket    59 Apr 18 20:10 jacket</span><br></pre></td></tr></table></figure><p>(2) 创建用户组：groupadd 组名<br>  groupadd bigdata<br>  添加用户到xx用户组： usermod    -a -G 组名 用户名<br>添加用户到bigdata用户组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# groupadd bigdata</span><br><span class="line">[root@learn01 ~]# usermod    -a -G bigdata      jacket</span><br><span class="line">[root@learn01 ~]# id jacket</span><br><span class="line">uid=1004(jacket) gid=1004(jacket) groups=1004(jacket),1005(bigdata)</span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# usermod -g bigdata jacket</span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[rroot@learn01 ~]# id jacket</span><br><span class="line">uid=1004(jacket) gid=1005(bigdata) groups=1005(bigdata)</span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# usermod    -a -G jacket      jacket</span><br><span class="line">[root@learn01 ~]# </span><br><span class="line">[root@learn01 ~]# id ruoze</span><br><span class="line">uid=1004(jacket) gid=1005(bigdata) groups=1005(bigdata),1004(jacket)</span><br></pre></td></tr></table></figure><p>(3)删除用户 :userdel 用户名<br>  userdel jacket<br>  删除、恢复样式<br>  su - jacket --&gt;ll -a --&gt;rm -rf .bash* --&gt;exit–&gt;cp /etc/skel/.* ./ --&gt;exit–&gt;su - jacket<br><strong>样式丢失</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# userdel jacket</span><br><span class="line">[root@learn01 ~]#  useradd jacket</span><br><span class="line">useradd: warning: the home directory already exists.</span><br><span class="line">Not copying any file from skel directory into it.</span><br><span class="line">Creating mailbox file: File exists</span><br><span class="line">[root@learn01 ~]#</span><br><span class="line">[root@learn01 ~]#  su - jacket</span><br><span class="line">Last login: Sun Apr 19 20:16:54 CST 2018 on pts/0</span><br><span class="line">[jacket@learn01 ~]$ ll -a</span><br><span class="line">total 16</span><br><span class="line">drwx------  2 jacket jacket  79 Apr 19 20:16 .</span><br><span class="line">drwxr-xr-x. 7 root  root   67 Apr 19 20:10 ..</span><br><span class="line">-rw-------  1 jacket jacket  28 Apr 19 20:17 .bash_history</span><br><span class="line">-rw-r--r--  1 jacket jacket  18 Apr 11  2018 .bash_logout</span><br><span class="line">-rw-r--r--  1 jacket jacket 193 Apr 11  2018 .bash_profile</span><br><span class="line">-rw-r--r--  1 jacket jacket 231 Apr 11  2018 .bashrc</span><br><span class="line">[jacket@learn01 ~]$ rm -rf .bash*</span><br><span class="line">[jacket@learn01 ~]$ ll -a</span><br><span class="line">total 0</span><br><span class="line">drwx------  2 jacket jacket  6 Apr 18 20:17 .</span><br><span class="line">drwxr-xr-x. 7 root  root  67 Apr 18 20:10 ..</span><br><span class="line">[jacket@learn01 ~]$ </span><br><span class="line">[jacket@learn01 ~]$ </span><br><span class="line">[jacket@learn01 ~]$ exit</span><br><span class="line">logout</span><br><span class="line">[root@learn01 ~]# su - jacket</span><br><span class="line">Last login: Sun Apr 19 20:17:08 CST 2018 on pts/0</span><br><span class="line">-bash-4.2$ </span><br><span class="line">-bash-4.2$ </span><br><span class="line">-bash-4.2$ </span><br><span class="line">-bash-4.2$ id</span><br><span class="line">uid=1004(jacket) gid=1004(jacket) groups=1004(jacket)</span><br><span class="line">-bash-4.2$ </span><br><span class="line">-bash-4.2$ cp /etc/skel/.* ./</span><br><span class="line">cp: omitting directory ‘/etc/skel/.’</span><br><span class="line">cp: omitting directory ‘/etc/skel/..’</span><br><span class="line">-bash-4.2$ ll -a</span><br><span class="line">total 16</span><br><span class="line">drwx------  2 jacket jacket  79 Apr 19 20:20 .</span><br><span class="line">drwxr-xr-x. 7 root  root   67 Apr 19 20:10 ..</span><br><span class="line">-rw-------  1 jacket jacket  39 Apr 19 20:18 .bash_history</span><br><span class="line">-rw-r--r--  1 jacket jacket  18 Apr 19 20:20 .bash_logout</span><br><span class="line">-rw-r--r--  1 jacket jacket 193 Apr 19 20:20 .bash_profile</span><br><span class="line">-rw-r--r--  1 jacket jacket 231 Apr 19 20:20 .bashrc</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><p><strong>样式恢复</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-bash-4.2$ </span><br><span class="line">-bash-4.2$ exit</span><br><span class="line">logout</span><br><span class="line">[root@learn01 ~]# su - jacket</span><br><span class="line"> Last login: Sun Apr 19 20:18:56 CST 2018 on pts/0</span><br></pre></td></tr></table></figure><h4 id="3设置密码passwd">3.设置密码passwd</h4><p>passwd 直接回车 修改root密码<br>passwd 用户名 修改对应用户密码</p><h4 id="4切换用户">4.切换用户</h4><p>(a) su 用户名（不建议不会执行环境文件）<br>(b) su - 用户名(推荐)代表该用户切换到家目录，且执行环境变量文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@learn01 ~]# pwd</span><br><span class="line">/root</span><br><span class="line">[root@learn01 ~]# su jacket</span><br><span class="line">[jacket@learn01 root]$ pwd</span><br><span class="line">/root</span><br><span class="line">[jacket@learn01 root]$ exit</span><br><span class="line">exit</span><br><span class="line">[root@learn01 ~]# su - jacket</span><br><span class="line">Last login: Sun Apr 19 20:33:23 CST 2018 on pts/2</span><br><span class="line">[jacket@learn01 ~]$ pwd</span><br><span class="line">/home/jacket</span><br></pre></td></tr></table></figure><h4 id="5sudo-普通用户临时使用root的最大权限">5.sudo 普通用户临时使用root的最大权限</h4><p>vi /etc/sudoers  添加：jacket   ALL=(root)      NOPASSWD:ALL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[jacket@learn01 ~]$ cat /root/rz.log</span><br><span class="line">cat: /root/rz.log: Permission denied</span><br><span class="line">[jacket@learn01 ~]$</span><br><span class="line">[jacket@learn01 ~]$ sudo cat /root/rz.log</span><br><span class="line">hello world</span><br><span class="line">[jacket@learn01 ~]$</span><br></pre></td></tr></table></figure><h4 id="6etcpasswd文件">6./etc/passwd文件</h4><p>tail -2 /etc/passwd<br>jacket:x:1004:1005::/home/jacket:/sbin/nologin  提示<br>jacket:x:1004:1005::/home/jacket:/usr/bin/false 没提示<br>CDH平台  hdfs yarn hive hbase<br>su - yarn 不成功的<br>/sbin/nologin /usr/bin/false ===》/bin/bash</p><h4 id="7权限">7.权限</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[jacket@learn01 ~]# ll</span><br><span class="line">-rw-r--r-- 1 root root  9 Apr 18 20:50 22.log</span><br><span class="line">drwxr-xr-x 2 root root  6 Apr 15 22:12 dir3</span><br></pre></td></tr></table></figure><p>drwxrwxr-x  第一位：d代表目录  -代表文件<br>后面九位，三个一组：rwx(所有者的权限)-&gt; u（user）       rwx(所属组的权限) -&gt;g（group）  r-x(其他用户权限) -&gt; o（other）rwx<br>  r: read  读权限 4  100(二进制)<br>  w: write 写权限 2   10(二进制)<br>  x: execute执行  1     1(二进制)<br>  -: 没权限       0  占位<br><strong>关于修改权限:</strong><br>chmod -R 777          文件或文件夹<br>chown -R 用户:用户组  文件或文件夹<br>案例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[jacket@learn01 tmp]# vi jacket.log</span><br><span class="line">hello world</span><br><span class="line">[jacket@learn01 tmp]$ cat  jacket.log </span><br><span class="line">hello word</span><br><span class="line">[jacket@learn01 tmp]$ </span><br><span class="line">收回其他组的r权限 </span><br><span class="line">[root@learn01 tmp]# chmod 640 jacket.log</span><br><span class="line">[jacket@learn01 tmp]$ cat  jacket.log </span><br><span class="line">cat: jacket.log: Permission denied</span><br></pre></td></tr></table></figure><h4 id="8查看文件或文件夹大小">8.查看文件或文件夹大小</h4><p>8.1 文件： ll -h 、du -sh<br>8.2 文件夹:        du -sh</p><h4 id="9搜索-find">9.搜索 find</h4><p>接手大数据平台，服务器登录，大数据组件安装目录在哪？<br>find / -name '*hadoop*'<br>find /home/hadoop/ -name ‘*hadoop*’</p><p>补充:<br>history 命令<br>ps -ef 查看进程</p><h4 id="10vi命令">10.vi命令</h4><p>10.1 正常编辑一个文件，要正常退出 wq<br>  反之(）:<font color="LimeGreen"> 坑1</font></p><blockquote><p>-rw-r–r--   1 root  root       16 Apr 19 21:26 2.log<br>-rw-r–r--   1 root  root    12288 Apr 19 21:31 .2.log.swp<br>需要 rm -rf .2.log.swp</p></blockquote><p>10.2 粘贴的<font color="LimeGreen"> 坑2</font>，必须进入编辑模式i，否则第一行内容丢失 不完整<br>10.3 搜索 尾行模式–》 /error<br>10.4设置或取消行号<br>  尾行–》 set nu   设置行号<br>  set nonu 取消行号<br>  f 也是可以显示 当前光标的所在的行<br>10.5 常用快捷方式<br>  dd 删除当前行<br>  dG 删除当前及以下所有行<br>  ndd 删除当前及以下n行<br>  gg 跳转到第一行的第一个字母<br>  G  跳转到最后一行的第一个字母<br>  shift+$ 行尾</p><p><strong>场景:</strong><br>清空这个文件内容，从另外一个文件内容 拷贝过来<br>gg–》dG   --》 i  --&gt;鼠标右键单击 粘贴上<br>ggdG 清空</p><p>清空补充：<br>(i)cat /dev/null &gt; 1.log<br>(ii)<strong>注意</strong> echo  “” &gt; 2.log (有字节，不是0字节不建议）</p><blockquote><p>[root@learn01 ~]# ll<br>total 16<br>-rw-r–r-- 1 root  root     0 Apr 18 21:58 1.log<br>-rw-r–r-- 1 root  root     1 Apr 18 21:58 2.log</p></blockquote><p><strong>场景:</strong><br>shell脚本，数据文件清空操作，根据字节大小判断是否清空完成<br>echo  “” &gt; 2.log<br>if filezie &gt; 0 then<br>  业务不操作<br>else<br>  2.log 灌业务数据</p><p>(iii)true &gt; 1.log 也是清空文件内容 0字节</p>]]></content>
      
      
      
        <tags>
            
            <tag> Linux03 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux02命令</title>
      <link href="/article/linux02%E5%91%BD%E4%BB%A4/"/>
      <url>/article/linux02%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h4 id="1清屏">1.清屏</h4><p>clear或快捷键ctrl + l</p><h4 id="2移动mv-复制cp命令">2.移动(mv)、复制(cp)命令</h4><p>2.1 mv [source] [target] (mv是始终一份  快)<br> eg:不标准写法:mv dir1  data<br> eg:标准写法:mv dir1  data/dir1 支持改名mv dir1  data/dir111<br>2.2 cp [-R/r]  [原文件或目录][目标目录](cp是两份  慢)<br> eg:不标准写法:cp -r dir2 data<br> eg:标准写法:cp -r dir2 data/dir2 支持改名mv dir2  data/dir222</p><h4 id="3创建文件或目录">3.创建文件或目录</h4><p>3.1 创建文件（三种方法如下）<br>（a）touch 空文件<br>eg:touch test.txt<br>（b）vi  1.log (详见vi编辑器)</p><blockquote><p>默认命令行<br>i键 编辑模式，进行编辑<br>esc键 从编辑模式–》命令行模式<br>shift+:键 从命令行模式–》尾行模式，输入wq 保存退出</p></blockquote><hr><p><strong>© echo “data” &gt; 2.log 创建或覆盖 <font color="red"> 【高危】 </font></strong></p><p>3.2 mkdir [-p] 目录<br>  eg:mkdir dir1 dir2 dir3 创建三个目录（并联创建）<br>  eg:mkdir -p dir4/dir5/dir6( 串 级联创建) ,加-p原本目录下创建目录（如无原本目录，同时创建原本目录</p><h4 id="4vi文件编辑器创建或编辑已有">4.vi文件编辑器(创建或编辑已有）</h4><p>4.1 eg: vi   ts.log</p><blockquote><p>1.命令模式 默认命令行<br>2.编辑模式(按i或a进入)（按Esc退出 从编辑模式–》命令行模式）<br>3.尾行模式 (shift+:键 从命令行模式–》尾行模式)<br>     wq!强制保存并退出<br>   w 保存<br>   w!  强制保存<br>   q 退出<br>   q!  强制退出</p></blockquote><p>4.2 vi 查找搜索<br>  vi xxx.log<br><em>  shift+: / ERROR(查找error关键字)</em></p><h4 id="5查看文件内容">5.查看文件内容</h4><p>5.1 cat<br>  cat [-n] 文件名<br>   -n 显示行号<br>   查看文件内容（推荐查看小文件）ctrl+z 中断</p><blockquote><p>cat 文件内容超多  定位ERROR 信息<br>cat xxx.log | grep ERROR   当前行<br>cat xxx.log | grep -A 5 ERROR   后5行<br>cat xxx.log | grep -B 5 ERROR   前5行<br>cat xxx.log | grep -C 5 ERROR   前后各5行</p></blockquote><p>5.2 more<br>  more 文件名<br>   空格或f 翻页（一页一页的往后显示）<br>5.3 less<br>  less 文件名<br>   按上下键   q退出<br>5.4 head<br>  head -n [文件名]<br>   查看文件的前n（正整数）行<br>   -不加-n选项，表示默认查看前10行的内容<br>5.5 tail 实时查看<br>  tail -f xxx.log （中途中断易丢失数据）<br>  tail -F yyy.log   =-f+ retry（大数据Flume组件 exec source ）</p><blockquote><p>场景: 采集业务log日志内容 log4j<br>规则: 每份100m 保留10份<br>系统–》 erp.log 90m …100m<br>  mv erp.log  erp.log1(数据满后，移动新建文件名继续接收新数据）<br>  touch erp.log<br>  ll命令查看<br>  erp.log<br>  erp.log1<br>  erp.log2<br>  …<br>  erp.log10</p></blockquote><hr><ul><li>查找错误日志<br>(a) tail -100F   xxx.log  错误的<br>(b) cat xxx.log | grep -C 5 ERROR 如 xxx.log ERROR很多  上万个<br>  cat xxx.log | grep -C 5 ERROR &gt; 20200418error.log<br>  more 20200418error.log<br>©vi xxx.log 文件内容100m   :<br>  vi xxx.log<br>  shift+:<br>    /<br>    ERROR<br>    n寻找<br>    下载到window<br>    editplus（window）<br>    notepad++<br>    sublime（widnow  mac）工具 搜索  统计 校验</li></ul><h4 id="6echo-命令">6.echo 命令</h4><p>  (a) echo “hello world” 打印命令<br>  (b) echo “data” &gt;&gt; 3.log 追加<br>  © echo “data” &gt; 3.log 创建或覆盖 <font color="red"> 【高危】同上3.1（c） </font></p><h4 id="7rzsz上传下载">7.rzsz上传下载</h4><p>  安装工具包 yum install -y lrzsz<br>  sz xxx.log是下载 Linux–》window<br>  rz 是上传   window–》Linux</p><h4 id="8alias别名">8.alias别名</h4><p>alias 回车查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">alias cp=&apos;cp -i&apos;</span><br><span class="line">alias egrep=&apos;egrep --color=auto&apos;</span><br><span class="line">alias fgrep=&apos;fgrep --color=auto&apos;</span><br><span class="line">alias grep=&apos;grep --color=auto&apos;</span><br><span class="line">alias l.=&apos;ls -d .* --color=auto&apos;</span><br><span class="line">alias ll=&apos;ls -l --color=auto&apos;</span><br><span class="line">alias ls=&apos;ls --color=auto&apos;</span><br><span class="line">alias mv=&apos;mv -i&apos;</span><br><span class="line">alias rm=&apos;rm -i&apos;</span><br></pre></td></tr></table></figure><p>建alias jj='cd /tmp’<br>当前窗口创建临时生效<br>vi /etc/profile配置永久生效(所有用户可用）私用~/.bashrc</p><h4 id="9环境变量">9.环境变量</h4><p>全局:<br>/etc/profile 所有用户都可以使用<br>个人:<br>~/.bash_profile  ssh 远程执行B机器 命令  找不到<br>~/.bashrc<font color="blue"> (推荐)</font> ssh 远程 配置在.bashrc文件<br>生效文件<br>source /etc/profile<br>source ~/.bash_profile<br>source ~/.bashrc或 . .bashrc</p><h4 id="10创建用户设置密码">10.创建用户/设置密码</h4><p>(a) 创建用户:useradd 用户名<br>  root 默认管理员用户 已存在的<br>  jacket</p><blockquote><p>[root@rzdata001 tmp]# useradd jacket<br>[root@rzdata001 tmp]# su - jacket<br>[jacket@rzdata001 ~]$ pwd<br>/home/jacket<br>[jacket@rzdatau001 ~]$<br>useradd --help<br>-G, --groups GROUPS<br>-U, --user-group<br>-u, --uid UID</p></blockquote><p>(b)设置密码:passwd 用户名<br>  passwd 回车 root密码设置<br>  passwd 用户名 对应用户密码设置</p><h4 id="11history-历史命令">11.history 历史命令</h4><p>!2  执行第二行命令<br>history -c 清空历史<br>场景：<br>  (a)直连 服务器 ok<br>  (b)跳板机(vpn)  服务器   ok<br>  ©堡垒机  敲一个命令 都记录</p><h4 id="12rm删除命令-高危命令">12.rm删除命令<font color="red"> 【高危命令】</font></h4><p>(a) ！！！不要做rm -rf /（所有数据清空）<br>(b) 脚本rm注意<br>场景:<br>  脚本里<br>  LOG_PATH=/xxx/yyy<br>  业务逻辑判断 去赋值<br>  漏了一种  没有赋值<br>  rm -rf ${LOG_PATH}/*  ==》rm -rf /*<br>如何避免：每次删除之前 都判断${LOG_PATH} 是否存在</p><h4 id="13hostnamectl命令">13.hostnamectl命令</h4><p>步骤一：hostnamectl set-hostname learn001<br>步骤二：ifconfig 查看IP<br>步骤三：vi /etc/hosts  添加ip hostname</p><blockquote><p>[root@JD ~]# hostnamectl set-hostname ruozedata001<br>[root@rzdata001 ~]# ifconfig 找内网ip 或者 京东云控制台查看<br>[root@rzdata001 ~]# vi /etc/hosts<br>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br>192.168.0.8 rzdata001(添加这一行）</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Linux02 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux01命令</title>
      <link href="/article/linux01%E5%91%BD%E4%BB%A4/"/>
      <url>/article/linux01%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h4 id="1root用户了解">1.root用户了解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@learn04 ~]#</span><br></pre></td></tr></table></figure><p>root： 默认管理员(超级管理员)  最大权限<br>learn04： 机器名称 pwd 查看当前所在路径<br>~：当前该用户的家目录用  /root<br>#     代表的当前用户为root<br>$：代表当前是普通用户操作</p><h4 id="2ls查看命令">2.ls查看命令</h4><p>ls 显示文件夹及文件<br>ls -l 长格式 显示权限 文件数 用户 用户组 文件大小 时间 文件名<br>ls -l -a 显示隐藏文件 文件夹<br>ls -l -h 仅仅查看文件大小<br>ls -l -r -t 按时间排序（快速找到排序文件-r sort -t time）<br><strong>等价</strong><br>ls -1 &lt;=&gt; ll<br>ls -l -a  &lt;=&gt; ll -a<br>ls -l -h &lt;=&gt; ll -h<br>ls -l -r -t &lt;=&gt; ll -rt</p><h4 id="3mkdir-创建目录命令">3.mkdir 创建目录命令</h4><p>mkdir [-p] dirName<br>mkdir dir1 dir2 dir3 创建三个目录（并联创建）<br>mkdir -p dir4/dir5/dir6( 串 级联创建) ,加-p原本目录下创建目录（如无原本目录，同时创建原本目录）</p><h4 id="4cd切换目录路径命令">4.cd切换目录路径命令</h4><p>cd / 切换到根目录<br>cd …/  回到上一级目录<br>cd …/…/  返回上两级目录<br>cd - 回退到上一次的目录<br>cd ~切换到用户主目录<br>root用户家目录切换三种方法：<br>* cd /root<br>* cd 直接回车 (推荐）<br>* cd ~<br>普通用户家目录切换三种方法：<br>* cd /home/xx<br>* cd 直接回车<br>* cd ~</p><h4 id="5help命令帮助">5.help命令帮助</h4><p>[commmand] --help 命令帮助查询使用参数<br>eg：ls --help Usage: ls [OPTION]… [FILE]… []标识的  可选  …  多个参数</p>]]></content>
      
      
      
        <tags>
            
            <tag> Linux01 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
